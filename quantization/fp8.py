"""
FP8 é‡åŒ–åŠ è½½
åŸºäº ComfyUI-WanVideoWrapper çš„å®ç°
éœ€è¦ ~24GB æ˜¾å­˜ï¼Œéœ€è¦ Compute Capability >= 8.0
"""
import torch
import torch.nn as nn


def check_fp8_support():
    """æ£€æŸ¥å½“å‰ç¡¬ä»¶æ˜¯å¦æ”¯æŒ FP8 è®¡ç®—
    
    Returns:
        tuple: (supports_fp8, compute_capability, message)
    """
    if not torch.cuda.is_available():
        return False, None, "CUDA ä¸å¯ç”¨"
    
    major, minor = torch.cuda.get_device_capability()
    compute_cap = f"{major}.{minor}"
    
    # FP8 matmul éœ€è¦ CUDA compute capability >= 8.9 (RTX 4000 ç³»åˆ—åŠä»¥ä¸Š)
    if (major, minor) >= (8, 9):
        return True, compute_cap, f"å®Œå…¨æ”¯æŒ FP8 (Compute Capability {compute_cap})"
    elif (major, minor) >= (8, 0):
        return True, compute_cap, f"éƒ¨åˆ†æ”¯æŒ FP8 (Compute Capability {compute_cap})ï¼Œæ¨è RTX 4000+ ç³»åˆ—"
    else:
        return False, compute_cap, f"ä¸æ”¯æŒ FP8 (Compute Capability {compute_cap})ï¼Œéœ€è¦ >= 8.0"


def fp8_linear_forward(cls, base_dtype, input):
    """FP8 Linear å±‚çš„ forward å‡½æ•°
    
    ä½¿ç”¨ torch._scaled_mm è¿›è¡Œ FP8 çŸ©é˜µä¹˜æ³•
    """
    weight_dtype = cls.weight.dtype
    
    if weight_dtype in [torch.float8_e4m3fn, torch.float8_e5m2]:
        if len(input.shape) == 3:
            input_shape = input.shape

            # è·å–æˆ–åˆ›å»º scale_weight
            scale_weight = getattr(cls, 'scale_weight', None)
            if scale_weight is None:
                scale_weight = torch.ones((), device=input.device, dtype=torch.float32)
            else:
                scale_weight = scale_weight.to(input.device).squeeze()

            scale_input = torch.ones((), device=input.device, dtype=torch.float32)

            # Clamp è¾“å…¥åˆ° FP8 e4m3fn çš„æœ‰æ•ˆèŒƒå›´
            input = torch.clamp(input, min=-448, max=448, out=input)
            
            # è½¬æ¢è¾“å…¥ä¸º FP8
            inn = input.reshape(-1, input_shape[2]).to(torch.float8_e4m3fn).contiguous()

            bias = cls.bias.to(base_dtype) if cls.bias is not None else None

            # ä½¿ç”¨ scaled_mm è¿›è¡Œ FP8 çŸ©é˜µä¹˜æ³•
            o = torch._scaled_mm(
                inn, 
                cls.weight.t(), 
                out_dtype=base_dtype, 
                bias=bias, 
                scale_a=scale_input, 
                scale_b=scale_weight
            )

            return o.reshape((-1, input_shape[1], cls.weight.shape[0]))
        else:
            return cls.original_forward(input.to(base_dtype))
    else:
        return cls.original_forward(input)


def convert_fp8_linear(module, base_dtype, params_to_keep=None, scale_weight_keys=None):
    """å°†æ¨¡å‹ä¸­çš„ Linear å±‚è½¬æ¢ä¸º FP8 ä¼˜åŒ–ç‰ˆæœ¬
    
    Args:
        module: è¦è½¬æ¢çš„æ¨¡å‹
        base_dtype: åŸºç¡€æ•°æ®ç±»å‹ï¼ˆè¾“å‡ºç±»å‹ï¼‰
        params_to_keep: éœ€è¦ä¿æŒåŸç²¾åº¦çš„å‚æ•°åå…³é”®å­—é›†åˆ
        scale_weight_keys: scale_weight å­—å…¸
    """
    if params_to_keep is None:
        params_to_keep = {
            "norm", "bias", "time_in", "patch_embedding", "time_", 
            "img_emb", "modulation", "text_embedding", "adapter", 
            "add", "ref_conv", "audio_proj"
        }
    
    print("ğŸ”§ å¯ç”¨ FP8 çŸ©é˜µä¹˜æ³•ä¼˜åŒ–...")
    converted_count = 0
    skipped_count = 0
    
    for name, submodule in module.named_modules():
        if any(keyword in name for keyword in params_to_keep):
            skipped_count += 1
            continue
            
        if isinstance(submodule, nn.Linear):
            if scale_weight_keys is not None:
                scale_key = f"{name}.scale_weight"
                if scale_key in scale_weight_keys:
                    setattr(submodule, "scale_weight", scale_weight_keys[scale_key].float())
            
            original_forward = submodule.forward
            setattr(submodule, "original_forward", original_forward)
            setattr(submodule, "forward", 
                    lambda input, m=submodule: fp8_linear_forward(m, base_dtype, input))
            converted_count += 1
    
    print(f"   âœ… å·²è½¬æ¢ {converted_count} ä¸ª Linear å±‚ä¸º FP8")
    print(f"   â­ï¸  è·³è¿‡ {skipped_count} ä¸ªç‰¹æ®Šå±‚ï¼ˆä¿æŒåŸç²¾åº¦ï¼‰")


def load_fp8(pipe, repo_id, device, dtype):
    """FP8 é‡åŒ–åŠ è½½
    
    Args:
        pipe: ModularPipeline å®ä¾‹
        repo_id: æ¨¡å‹ä»“åº“ ID
        device: ç›®æ ‡è®¾å¤‡
        dtype: åŸºç¡€æ•°æ®ç±»å‹
    
    Returns:
        åŠ è½½å®Œæˆçš„ pipe
    """
    print("ğŸ”§ ä½¿ç”¨ FP8 ä¼˜åŒ– (åŸºäº ComfyUI å®ç°)...")
    
    from huggingface_hub import hf_hub_download
    from safetensors.torch import load_file
    from diffusers import AutoModel
    
    # æ£€æŸ¥ç¡¬ä»¶æ”¯æŒ
    supports_fp8, compute_cap, msg = check_fp8_support()
    print(f"   GPU: {msg}")
    if not supports_fp8:
        raise RuntimeError(f"å½“å‰ GPU ä¸æ”¯æŒ FP8: {msg}")
    
    # 1. ä¸‹è½½ FP8 checkpoint
    fp8_repo = "6chan/krea-realtime-video-fp8"
    fp8_file = "krea-realtime-video-14b-fp8-e4m3fn.safetensors"
    print(f"   [1/4] ä¸‹è½½ FP8 æƒé‡: {fp8_repo}")
    
    fp8_path = hf_hub_download(repo_id=fp8_repo, filename=fp8_file)
    print(f"   âœ… å·²ä¸‹è½½: {fp8_path}")
    
    # 2. åŠ è½½å…¶ä»–ç»„ä»¶ï¼ˆä¸åŒ…æ‹¬ transformerï¼‰
    print("   [2/4] åŠ è½½å…¶ä»–ç»„ä»¶ (VAE, Text Encoder)...")
    config_only_components = {"transformer", "guider", "video_processor", "scheduler"}
    specs = pipe._component_specs
    if isinstance(specs, dict):
        all_component_names = list(specs.keys())
    elif specs:
        first = next(iter(specs), None)
        if hasattr(first, 'name'):
            all_component_names = [spec.name for spec in specs]
        else:
            all_component_names = list(specs)
    else:
        all_component_names = []
    
    components_to_load = [name for name in all_component_names if name not in config_only_components]
    
    pipe.load_components(
        names=components_to_load,
        trust_remote_code=True,
        device_map=device,
        torch_dtype={"default": dtype, "vae": torch.float16},
    )
    
    # 3. åŠ è½½ FP8 æƒé‡
    print("   [3/4] åŠ è½½ FP8 æƒé‡...")
    fp8_state_dict = load_file(fp8_path)
    
    # æå– scale_weights
    scale_weights = {}
    for k, v in fp8_state_dict.items():
        if k.endswith(".scale_weight") or k.endswith(".weight_scale"):
            scale_weights[k.replace(".weight_scale", ".scale_weight")] = v.to(device, torch.float32)
    
    # éœ€è¦ä¿æŒåŸç²¾åº¦çš„å±‚ï¼ˆä¸ä½¿ç”¨ FP8ï¼‰
    params_to_keep = {
        "norm", "bias", "time_in", "patch_embedding", "time_", 
        "img_emb", "modulation", "text_embedding"
    }
    
    # 4. åŠ è½½ transformer ç»“æ„åˆ° CPUï¼Œç„¶åæ›¿æ¢æƒé‡
    print("   [4/4] åŠ è½½ transformer å¹¶æ›¿æ¢æƒé‡...")
    
    # å…ˆåŠ è½½åˆ° CPUï¼ˆé¿å… OOMï¼‰
    transformer = AutoModel.from_pretrained(
        repo_id,
        subfolder="transformer",
        torch_dtype=dtype,
        trust_remote_code=True,
        device_map="cpu",
    )
    
    # æ„å»º module name -> module çš„æ˜ å°„
    module_dict = {name: module for name, module in transformer.named_modules()}
    
    # éå† state_dict çš„æ‰€æœ‰é”®ï¼Œæ›¿æ¢æƒé‡
    loaded_fp8_count = 0
    loaded_bf16_count = 0
    
    for key, value in fp8_state_dict.items():
        # è·³è¿‡ scale_weight
        if "scale_weight" in key or "weight_scale" in key:
            continue
        
        # è§£æ key
        parts = key.rsplit(".", 1)
        if len(parts) == 2:
            module_name, param_name = parts
        else:
            module_name, param_name = "", key
        
        # åˆ¤æ–­æ˜¯å¦ä¿æŒåŸç²¾åº¦
        keep_original = any(keyword in key for keyword in params_to_keep)
        
        # åˆ¤æ–­æ˜¯å¦æ˜¯ Linear å±‚çš„ weightï¼ˆéœ€è¦ä¿æŒ FP8ï¼‰
        is_linear_weight = False
        if module_name in module_dict:
            module = module_dict[module_name]
            is_linear_weight = isinstance(module, nn.Linear) and param_name == "weight"
        
        # å†³å®šç›®æ ‡ dtype å’Œè®¾å¤‡
        if is_linear_weight and not keep_original:
            # Linear å±‚çš„ weight ä¿æŒ FP8 æ ¼å¼ï¼Œç›´æ¥æ”¾åˆ° GPU
            target_value = value.to(device)
            loaded_fp8_count += 1
        else:
            # å…¶ä»–æ‰€æœ‰å‚æ•°è½¬æ¢ä¸º bf16ï¼Œæ”¾åˆ° GPU
            target_value = value.to(device, dtype)
            loaded_bf16_count += 1
        
        # è®¾ç½®å‚æ•°
        if module_name == "" and hasattr(transformer, param_name):
            setattr(transformer, param_name, nn.Parameter(target_value, requires_grad=False))
        elif module_name in module_dict:
            module = module_dict[module_name]
            setattr(module, param_name, nn.Parameter(target_value, requires_grad=False))
    
    print(f"   âœ… å·²åŠ è½½ {loaded_fp8_count} ä¸ª FP8 å‚æ•° + {loaded_bf16_count} ä¸ª BF16 å‚æ•°")
    
    # æ¸…ç†æ˜¾å­˜
    del fp8_state_dict
    torch.cuda.empty_cache()
    
    # åº”ç”¨ FP8 Linear ä¼˜åŒ–
    convert_fp8_linear(transformer, dtype, params_to_keep, scale_weights)
    
    pipe.transformer = transformer
    
    # å°è¯•èåˆæŠ•å½±å±‚
    try:
        print("ğŸ”§ å°è¯•èåˆæŠ•å½±å±‚...")
        for block in pipe.transformer.blocks:
            block.self_attn.fuse_projections()
        print("   âœ… èåˆæˆåŠŸ")
    except Exception as e:
        print(f"   âš ï¸  è·³è¿‡ fuse_projections: {e}")
    
    torch.cuda.empty_cache()
    print("   âœ… FP8 ä¼˜åŒ–å®Œæˆ")
    
    return pipe
