"""
Text Encoder Offload è¾…åŠ©æ¨¡å—

ç”¨äºåœ¨é‡åŒ–æ¨¡å¼ä¸‹å°† Text Encoder å¸è½½åˆ° CPUï¼Œé‡Šæ”¾ GPU æ˜¾å­˜ç»™ KV cacheã€‚
Text Encoder (T5-XXL) çº¦å ç”¨ 10-12GB æ˜¾å­˜ã€‚
"""
import torch
import gc


class TextEncoderOffloadHelper:
    """Text Encoder Offload ç®¡ç†å™¨
    
    åœ¨é‡åŒ–æ¨¡å¼ä¸‹ï¼Œæ¨¡å‹åŠ è½½å Text Encoder ä»å ç”¨çº¦ 10GB æ˜¾å­˜ã€‚
    æ­¤ç±»æä¾›æ–¹æ³•åœ¨ prompt ç¼–ç å®Œæˆåå°† Text Encoder å¸è½½åˆ° CPUï¼Œ
    å¹¶åœ¨éœ€è¦ç¼–ç æ–° prompt æ—¶ä¸´æ—¶ç§»å› GPUã€‚
    """
    
    def __init__(self, pipe):
        self.pipe = pipe
        self.text_encoder_on_cpu = False
        self.original_device = None
        
        # æŸ¥æ‰¾ Text Encoder ç»„ä»¶
        self.text_encoder = None
        self.text_encoder_name = None
        
        # KREA æ¨¡å‹å¯èƒ½ä½¿ç”¨ä¸åŒçš„ Text Encoder åç§°
        possible_names = ['text_encoder', 'text_encoder_1', 'text_encoder_2', 't5_encoder']
        for name in possible_names:
            if hasattr(pipe, name) and getattr(pipe, name) is not None:
                self.text_encoder = getattr(pipe, name)
                self.text_encoder_name = name
                break
        
        if self.text_encoder is not None:
            # è·å–åŸå§‹è®¾å¤‡
            try:
                first_param = next(self.text_encoder.parameters())
                self.original_device = first_param.device
            except StopIteration:
                self.original_device = torch.device('cuda')
    
    def offload_to_cpu(self):
        """å°† Text Encoder å¸è½½åˆ° CPU
        
        åº”åœ¨ prompt ç¼–ç å®Œæˆåè°ƒç”¨ï¼ˆé€šå¸¸æ˜¯ç¬¬ä¸€ä¸ª block ç”Ÿæˆåï¼‰
        """
        if self.text_encoder is None:
            return
        
        if self.text_encoder_on_cpu:
            return  # å·²ç»åœ¨ CPU ä¸Š
        
        print(f"ğŸ“¤ Offloading {self.text_encoder_name} to CPU...")
        
        # ç§»åŠ¨åˆ° CPU
        self.text_encoder.to('cpu')
        self.text_encoder_on_cpu = True
        
        # æ¸…ç† GPU ç¼“å­˜
        gc.collect()
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        
        # æ‰“å°é‡Šæ”¾çš„æ˜¾å­˜
        allocated = torch.cuda.memory_allocated() / 1024 / 1024 / 1024
        print(f"   âœ… Done. GPU memory now: {allocated:.2f}GB")
    
    def restore_to_gpu(self):
        """å°† Text Encoder æ¢å¤åˆ° GPU
        
        åº”åœ¨éœ€è¦ç¼–ç æ–° prompt å‰è°ƒç”¨
        """
        if self.text_encoder is None:
            return
        
        if not self.text_encoder_on_cpu:
            return  # å·²ç»åœ¨ GPU ä¸Š
        
        print(f"ğŸ“¥ Restoring {self.text_encoder_name} to GPU...")
        
        device = self.original_device or torch.device('cuda')
        self.text_encoder.to(device)
        self.text_encoder_on_cpu = False
        
        torch.cuda.synchronize()
        print(f"   âœ… Done")
    
    def encode_prompt_with_offload(self, encode_fn, *args, **kwargs):
        """ç¼–ç  prompt å¹¶è‡ªåŠ¨å¤„ç† offload
        
        å¦‚æœ Text Encoder åœ¨ CPU ä¸Šï¼Œä¸´æ—¶ç§»åˆ° GPUï¼Œç¼–ç åç§»å› CPUã€‚
        
        Args:
            encode_fn: ç¼–ç å‡½æ•°
            *args, **kwargs: ä¼ é€’ç»™ç¼–ç å‡½æ•°çš„å‚æ•°
        
        Returns:
            ç¼–ç ç»“æœ
        """
        was_on_cpu = self.text_encoder_on_cpu
        
        if was_on_cpu:
            self.restore_to_gpu()
        
        try:
            result = encode_fn(*args, **kwargs)
        finally:
            if was_on_cpu:
                self.offload_to_cpu()
        
        return result


def setup_text_encoder_offload(pipe):
    """ä¸º pipeline è®¾ç½® Text Encoder Offload
    
    åœ¨é‡åŒ–æ¨¡å¼ä¸‹è°ƒç”¨ï¼Œå°† offload helper é™„åŠ åˆ° pipelineã€‚
    
    Args:
        pipe: ModularPipeline å®ä¾‹
    
    Returns:
        pipe (å¸¦æœ‰ offload_helper å±æ€§)
    """
    helper = TextEncoderOffloadHelper(pipe)
    pipe._text_encoder_offload_helper = helper
    
    if helper.text_encoder is not None:
        print(f"ğŸ”§ Text Encoder Offload å·²å¯ç”¨ ({helper.text_encoder_name})")
    else:
        print("âš ï¸  æœªæ‰¾åˆ° Text Encoderï¼Œè·³è¿‡ Offload è®¾ç½®")
    
    return pipe


def offload_text_encoder(pipe):
    """å¸è½½ Text Encoder åˆ° CPUï¼ˆä¾¿æ·å‡½æ•°ï¼‰"""
    if hasattr(pipe, '_text_encoder_offload_helper'):
        pipe._text_encoder_offload_helper.offload_to_cpu()


def restore_text_encoder(pipe):
    """æ¢å¤ Text Encoder åˆ° GPUï¼ˆä¾¿æ·å‡½æ•°ï¼‰"""
    if hasattr(pipe, '_text_encoder_offload_helper'):
        pipe._text_encoder_offload_helper.restore_to_gpu()
