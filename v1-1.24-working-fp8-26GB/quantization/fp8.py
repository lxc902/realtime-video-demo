"""
FP8 é‡åŒ–åŠ è½½
åŸºäº ComfyUI-WanVideoWrapper çš„å®ç°
éœ€è¦ ~24GB æ˜¾å­˜ï¼Œéœ€è¦ Compute Capability >= 8.0
"""
import torch
import torch.nn as nn


def check_fp8_support():
    """æ£€æŸ¥å½“å‰ç¡¬ä»¶æ˜¯å¦æ”¯æŒ FP8 è®¡ç®—
    
    Returns:
        tuple: (supports_fp8, compute_capability, message)
    """
    if not torch.cuda.is_available():
        return False, None, "CUDA ä¸å¯ç”¨"
    
    major, minor = torch.cuda.get_device_capability()
    compute_cap = f"{major}.{minor}"
    
    # FP8 matmul éœ€è¦ CUDA compute capability >= 8.9 (RTX 4000 ç³»åˆ—åŠä»¥ä¸Š)
    if (major, minor) >= (8, 9):
        return True, compute_cap, f"å®Œå…¨æ”¯æŒ FP8 (Compute Capability {compute_cap})"
    elif (major, minor) >= (8, 0):
        return True, compute_cap, f"éƒ¨åˆ†æ”¯æŒ FP8 (Compute Capability {compute_cap})ï¼Œæ¨è RTX 4000+ ç³»åˆ—"
    else:
        return False, compute_cap, f"ä¸æ”¯æŒ FP8 (Compute Capability {compute_cap})ï¼Œéœ€è¦ >= 8.0"


def fp8_linear_forward(cls, base_dtype, input):
    """FP8 Linear å±‚çš„ forward å‡½æ•°
    
    ä½¿ç”¨ torch._scaled_mm è¿›è¡Œ FP8 çŸ©é˜µä¹˜æ³•
    """
    weight_dtype = cls.weight.dtype
    
    if weight_dtype in [torch.float8_e4m3fn, torch.float8_e5m2]:
        # è·å–æˆ–åˆ›å»º scale_weight
        scale_weight = getattr(cls, 'scale_weight', None)
        if scale_weight is None:
            scale_weight = torch.ones((), device=input.device, dtype=torch.float32)
        else:
            scale_weight = scale_weight.to(input.device).squeeze()

        # å¤ç”¨é¢„åˆ›å»ºçš„ scale_inputï¼Œé¿å…æ¯æ¬¡ forward åˆ›å»ºæ–°å¼ é‡
        scale_input = getattr(cls, '_scale_input_cache', None)
        if scale_input is None or scale_input.device != input.device:
            scale_input = torch.ones((), device=input.device, dtype=torch.float32)
            cls._scale_input_cache = scale_input
        
        # Clamp è¾“å…¥åˆ° FP8 e4m3fn çš„æœ‰æ•ˆèŒƒå›´
        input = torch.clamp(input, min=-448, max=448)
        
        bias = cls.bias.to(base_dtype) if cls.bias is not None else None
        
        if len(input.shape) == 3:
            # 3D è¾“å…¥: [batch, seq, features]
            input_shape = input.shape
            inn = input.reshape(-1, input_shape[2]).to(torch.float8_e4m3fn).contiguous()
            
            o = torch._scaled_mm(
                inn, 
                cls.weight.t(), 
                out_dtype=base_dtype, 
                bias=bias, 
                scale_a=scale_input, 
                scale_b=scale_weight
            )
            return o.reshape((-1, input_shape[1], cls.weight.shape[0]))
        
        elif len(input.shape) == 2:
            # 2D è¾“å…¥: [batch, features]
            inn = input.to(torch.float8_e4m3fn).contiguous()
            
            o = torch._scaled_mm(
                inn, 
                cls.weight.t(), 
                out_dtype=base_dtype, 
                bias=bias, 
                scale_a=scale_input, 
                scale_b=scale_weight
            )
            return o
        
        else:
            # å…¶ä»–ç»´åº¦ï¼šå°†æƒé‡è½¬æ¢ä¸º base_dtype è¿›è¡Œè®¡ç®—
            weight_bf16 = cls.weight.to(base_dtype)
            return torch.nn.functional.linear(input.to(base_dtype), weight_bf16, bias)
    else:
        return cls.original_forward(input)


def convert_fp8_linear(module, base_dtype, params_to_keep=None, scale_weight_keys=None):
    """å°†æ¨¡å‹ä¸­çš„ Linear å±‚è½¬æ¢ä¸º FP8 ä¼˜åŒ–ç‰ˆæœ¬
    
    Args:
        module: è¦è½¬æ¢çš„æ¨¡å‹
        base_dtype: åŸºç¡€æ•°æ®ç±»å‹ï¼ˆè¾“å‡ºç±»å‹ï¼‰
        params_to_keep: éœ€è¦ä¿æŒåŸç²¾åº¦çš„å‚æ•°åå…³é”®å­—é›†åˆ
        scale_weight_keys: scale_weight å­—å…¸
    """
    if params_to_keep is None:
        params_to_keep = {
            "norm", "bias", "time_in", "patch_embedding", "time_", 
            "img_emb", "modulation", "text_embedding", "adapter", 
            "add", "ref_conv", "audio_proj"
        }
    
    print("ğŸ”§ å¯ç”¨ FP8 çŸ©é˜µä¹˜æ³•ä¼˜åŒ–...")
    converted_count = 0
    skipped_count = 0
    
    for name, submodule in module.named_modules():
        if any(keyword in name for keyword in params_to_keep):
            skipped_count += 1
            continue
            
        if isinstance(submodule, nn.Linear):
            if scale_weight_keys is not None:
                scale_key = f"{name}.scale_weight"
                if scale_key in scale_weight_keys:
                    setattr(submodule, "scale_weight", scale_weight_keys[scale_key].float())
            
            original_forward = submodule.forward
            setattr(submodule, "original_forward", original_forward)
            setattr(submodule, "forward", 
                    lambda input, m=submodule: fp8_linear_forward(m, base_dtype, input))
            converted_count += 1
    
    print(f"   âœ… å·²è½¬æ¢ {converted_count} ä¸ª Linear å±‚ä¸º FP8")
    print(f"   â­ï¸  è·³è¿‡ {skipped_count} ä¸ªç‰¹æ®Šå±‚ï¼ˆä¿æŒåŸç²¾åº¦ï¼‰")


def load_fp8(pipe, repo_id, device, dtype):
    """FP8 é‡åŒ–åŠ è½½
    
    Args:
        pipe: ModularPipeline å®ä¾‹
        repo_id: æ¨¡å‹ä»“åº“ ID
        device: ç›®æ ‡è®¾å¤‡
        dtype: åŸºç¡€æ•°æ®ç±»å‹
    
    Returns:
        åŠ è½½å®Œæˆçš„ pipe
    """
    print("ğŸ”§ ä½¿ç”¨ FP8 ä¼˜åŒ– (åŸºäº ComfyUI å®ç°)...")
    
    from huggingface_hub import hf_hub_download
    from safetensors.torch import load_file
    from diffusers import AutoModel
    
    # æ£€æŸ¥ç¡¬ä»¶æ”¯æŒ
    supports_fp8, compute_cap, msg = check_fp8_support()
    print(f"   GPU: {msg}")
    if not supports_fp8:
        raise RuntimeError(f"å½“å‰ GPU ä¸æ”¯æŒ FP8: {msg}")
    
    # 1. ä¸‹è½½ FP8 checkpoint
    fp8_repo = "6chan/krea-realtime-video-fp8"
    fp8_file = "krea-realtime-video-14b-fp8-e4m3fn.safetensors"
    print(f"   [1/4] ä¸‹è½½ FP8 æƒé‡: {fp8_repo}")
    
    fp8_path = hf_hub_download(repo_id=fp8_repo, filename=fp8_file)
    print(f"   âœ… å·²ä¸‹è½½: {fp8_path}")
    
    # 2. åŠ è½½å…¶ä»–ç»„ä»¶ï¼ˆä¸åŒ…æ‹¬ transformerï¼‰
    print("   [2/4] åŠ è½½å…¶ä»–ç»„ä»¶ (VAE, Text Encoder)...")
    config_only_components = {"transformer", "guider", "video_processor", "scheduler"}
    specs = pipe._component_specs
    if isinstance(specs, dict):
        all_component_names = list(specs.keys())
    elif specs:
        first = next(iter(specs), None)
        if hasattr(first, 'name'):
            all_component_names = [spec.name for spec in specs]
        else:
            all_component_names = list(specs)
    else:
        all_component_names = []
    
    components_to_load = [name for name in all_component_names if name not in config_only_components]
    
    pipe.load_components(
        names=components_to_load,
        trust_remote_code=True,
        device_map=device,
        torch_dtype={"default": dtype, "vae": torch.float16},
    )
    
    # 3. åŠ è½½ FP8 æƒé‡
    print("   [3/4] åŠ è½½ FP8 æƒé‡...")
    fp8_state_dict = load_file(fp8_path)
    
    # æå– scale_weightsï¼ˆå»æ‰ model. å‰ç¼€ï¼‰
    scale_weights = {}
    for k, v in fp8_state_dict.items():
        if k.endswith(".scale_weight") or k.endswith(".weight_scale"):
            normalized_k = k.replace(".weight_scale", ".scale_weight")
            if normalized_k.startswith("model."):
                normalized_k = normalized_k[6:]
            scale_weights[normalized_k] = v.to(device, torch.float32)
    
    # éœ€è¦ä¿æŒåŸç²¾åº¦çš„å±‚ï¼ˆä¸ä½¿ç”¨ FP8ï¼‰
    params_to_keep = {
        "norm", "bias", "time_in", "patch_embedding", "time_", 
        "img_emb", "modulation", "text_embedding"
    }
    
    # 4. åŠ è½½ transformer åˆ° CPUï¼Œæ›¿æ¢ FP8 æƒé‡ï¼Œå†ç§»åŠ¨åˆ° GPU
    print("   [4/4] åŠ è½½ transformer å¹¶æ›¿æ¢ FP8 æƒé‡...")
    
    # å…ˆåŠ è½½åˆ° CPUï¼ˆé¿å… OOMï¼‰
    transformer = AutoModel.from_pretrained(
        repo_id,
        subfolder="transformer",
        torch_dtype=dtype,
        trust_remote_code=True,
        device_map="cpu",
    )
    
    # FP8 dtype åˆ—è¡¨
    fp8_dtypes = [torch.float8_e4m3fn, torch.float8_e5m2]
    
    # è·å–æ¨¡å‹çš„ state_dict keys ç”¨äºåŒ¹é…
    model_state_dict = transformer.state_dict()
    
    # FP8 checkpoint çš„ key æœ‰ "model." å‰ç¼€ï¼Œéœ€è¦å»æ‰
    def normalize_key(key):
        if key.startswith("model."):
            return key[6:]  # å»æ‰ "model." å‰ç¼€
        return key
    
    # æ„å»º normalized key -> original key çš„æ˜ å°„
    fp8_key_map = {normalize_key(k): k for k in fp8_state_dict.keys()}
    
    # åªæ›¿æ¢ FP8 æƒé‡ï¼ˆLinear å±‚çš„ weightï¼‰
    replaced_count = 0
    for model_key in model_state_dict.keys():
        # æ£€æŸ¥ FP8 state_dict ä¸­æ˜¯å¦æœ‰å¯¹åº”çš„ keyï¼ˆå»æ‰ model. å‰ç¼€åï¼‰
        if model_key in fp8_key_map:
            fp8_key = fp8_key_map[model_key]
            fp8_value = fp8_state_dict[fp8_key]
            
            # åªæ›¿æ¢ FP8 æ ¼å¼çš„æƒé‡ï¼Œä¸”ä¸åœ¨ params_to_keep ä¸­
            is_fp8 = fp8_value.dtype in fp8_dtypes
            keep_original = any(keyword in model_key for keyword in params_to_keep)
            
            if is_fp8 and not keep_original:
                # è§£æ key æ‰¾åˆ°æ¨¡å—
                parts = model_key.rsplit(".", 1)
                if len(parts) == 2:
                    module_name, param_name = parts
                    # è·å–æ¨¡å—
                    module = transformer
                    for part in module_name.split("."):
                        module = getattr(module, part)
                    # æ›¿æ¢ä¸º FP8 æƒé‡ï¼ˆå…ˆä¿æŒåœ¨ CPUï¼‰
                    setattr(module, param_name, 
                            nn.Parameter(fp8_value, requires_grad=False))
                    replaced_count += 1
    
    print(f"   âœ… å·²æ›¿æ¢ {replaced_count} ä¸ªæƒé‡ä¸º FP8 æ ¼å¼")
    
    # è®¾ç½® scale_weights åˆ°æ¨¡å—ï¼ˆåœ¨ CPU ä¸Šï¼‰
    for k, v in fp8_state_dict.items():
        if k.endswith(".scale_weight") or k.endswith(".weight_scale"):
            module_key = k.replace(".scale_weight", "").replace(".weight_scale", "")
            if module_key.startswith("model."):
                module_key = module_key[6:]
            parts = module_key.rsplit(".", 1)
            if len(parts) == 2:
                module_name = parts[0]
                try:
                    module = transformer
                    for part in module_name.split("."):
                        module = getattr(module, part)
                    setattr(module, "scale_weight", v.float())
                except AttributeError:
                    pass
    
    # æ¸…ç† CPU å†…å­˜
    del fp8_state_dict
    del model_state_dict
    import gc
    gc.collect()
    
    # ç§»åŠ¨ transformer åˆ° GPU
    print("   ğŸ“¤ ç§»åŠ¨ transformer åˆ° GPU...")
    transformer = transformer.to(device)
    torch.cuda.empty_cache()
    torch.cuda.synchronize()
    print(f"   âœ… å·²ç§»åŠ¨åˆ° GPU")
    
    pipe.transformer = transformer
    
    # å°è¯•èåˆæŠ•å½±å±‚ï¼ˆåœ¨ FP8 ä¼˜åŒ–ä¹‹å‰ï¼Œå› ä¸ºèåˆä¼šåˆ›å»ºæ–°çš„ Linear å±‚ï¼‰
    try:
        print("ğŸ”§ å°è¯•èåˆæŠ•å½±å±‚...")
        for block in pipe.transformer.blocks:
            block.self_attn.fuse_projections()
        print("   âœ… èåˆæˆåŠŸ")
    except Exception as e:
        print(f"   âš ï¸  è·³è¿‡ fuse_projections: {e}")
    
    # åº”ç”¨ FP8 Linear ä¼˜åŒ–ï¼ˆåœ¨èåˆä¹‹åï¼Œè¿™æ ·å¯ä»¥åŒ…æ‹¬æ–°åˆ›å»ºçš„èåˆå±‚ï¼‰
    convert_fp8_linear(pipe.transformer, dtype, params_to_keep, scale_weights)
    
    torch.cuda.empty_cache()
    print("   âœ… FP8 ä¼˜åŒ–å®Œæˆ")
    
    # è®¾ç½® Text Encoder Offload å¹¶ç«‹å³æ‰§è¡Œï¼ˆç”¨äºé‡Šæ”¾æ˜¾å­˜ç»™ KV cacheï¼‰
    from .offload import setup_text_encoder_offload, offload_text_encoder
    pipe = setup_text_encoder_offload(pipe)
    offload_text_encoder(pipe)  # ç«‹å³ offloadï¼Œé‡Šæ”¾ ~10GB æ˜¾å­˜
    
    return pipe
