"""
KREA Realtime Video - æœ¬åœ° GPU ç‰ˆæœ¬
ä½¿ç”¨æœ¬åœ° GPU è€Œä¸æ˜¯ FAL API
"""
import os
import asyncio
import json
from typing import Optional
from fastapi import FastAPI, Request, WebSocket
from fastapi.responses import HTMLResponse
from fastapi.templating import Jinja2Templates
from fastapi.middleware.cors import CORSMiddleware
from starlette.middleware.base import BaseHTTPMiddleware
import msgpack

# å¯¼å…¥æœ¬åœ°æ¨ç†æ¨¡å—
from local_inference import get_model
from config import (
    MODEL_PATH, QUANTIZATION,
    V2V_INITIAL_FRAMES, V2V_SUBSEQUENT_FRAMES, FRAMES_PER_CHUNK,
    SESSION_TIMEOUT
)

app = FastAPI()
templates = Jinja2Templates(directory=".")

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Add CSP middleware
class CSPMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        response = await call_next(request)
        if "text/html" in response.headers.get("content-type", ""):
            response.headers["Content-Security-Policy"] = (
                "default-src 'self'; "
                "script-src 'self' 'unsafe-inline' 'unsafe-eval'; "
                "style-src 'self' 'unsafe-inline' https://fonts.googleapis.com; "
                "font-src 'self' https://fonts.gstatic.com; "
                "connect-src 'self' wss: https:; "
                "img-src 'self' data: blob:; "
                "media-src 'self' blob:;"
            )
        return response

app.add_middleware(CSPMiddleware)

# Track active WebSocket connections
active_websockets = set()

# å…¨å±€æ¨¡å‹å®ä¾‹
model = None

def load_model_on_startup():
    """å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹"""
    global model
    print("")
    print("=" * 60)
    print("ğŸ”¥ Loading KREA model to GPU...")
    if MODEL_PATH:
        print(f"   From: {MODEL_PATH}")
    else:
        print("   From: HuggingFace (krea/krea-realtime-video)")
    if QUANTIZATION:
        print(f"   Quantization: {QUANTIZATION.upper()}")
    else:
        print("   Quantization: None (full precision)")
    print("   This will take 1-2 minutes on first run")
    print("=" * 60)
    print("")
    model = get_model(model_path=MODEL_PATH, quantization=QUANTIZATION)
    print("")
    print("=" * 60)
    print("âœ… Model loaded successfully!")
    print("ğŸŒ Server is ready to accept connections")
    print("=" * 60)
    print("")

async def cleanup_expired_sessions():
    """åå°ä»»åŠ¡ï¼šå®šæœŸæ¸…ç†è¶…æ—¶çš„ HTTP sessions"""
    while True:
        await asyncio.sleep(30)  # æ¯ 30 ç§’æ£€æŸ¥ä¸€æ¬¡
        
        expired_sessions = []
        with session_lock:
            for session_id, session in list(active_sessions.items()):
                if session.is_expired():
                    expired_sessions.append((session_id, session))
        
        # æ¸…ç†è¶…æ—¶çš„ sessions
        for session_id, session in expired_sessions:
            with session_lock:
                if session_id in active_sessions:
                    # æ¸…ç†æ¨ç†æ˜¾å­˜
                    if hasattr(session.model, 'cleanup_inference'):
                        session.model.cleanup_inference()
                    del active_sessions[session_id]
                    print(f"[Cleanup] Session {session_id} expired and cleaned (timeout: {SESSION_TIMEOUT}s)")

@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶çš„äº‹ä»¶"""
    import asyncio
    # åœ¨åå°çº¿ç¨‹åŠ è½½æ¨¡å‹ï¼Œé¿å…é˜»å¡å¯åŠ¨
    loop = asyncio.get_event_loop()
    await loop.run_in_executor(None, load_model_on_startup)
    
    # å¯åŠ¨åå°æ¸…ç†ä»»åŠ¡
    asyncio.create_task(cleanup_expired_sessions())


@app.get("/", response_class=HTMLResponse)
async def home(request: Request):
    """Home page"""
    return templates.TemplateResponse("index.html", {"request": request})


@app.get("/health")
async def health():
    """Health check endpoint"""
    return {
        "status": "ok",
        "mode": "local_gpu",
        "model_loaded": model is not None,
        "ready": model is not None
    }


@app.post("/api/clear-cache")
async def clear_inference_cache():
    """æ¸…ç†æ¨ç†ç¼“å­˜ï¼ˆä¸å½±å“æ¨¡å‹åŠ è½½ï¼‰"""
    import gc
    
    cleaned_sessions = 0
    
    # æ¸…ç†æ‰€æœ‰æ´»è·ƒçš„ HTTP sessions
    with session_lock:
        for session_id, session in list(active_sessions.items()):
            if hasattr(session.model, 'cleanup_inference'):
                session.model.cleanup_inference()
            del active_sessions[session_id]
            cleaned_sessions += 1
    
    # æ¸…ç†å…¨å±€æ¨¡å‹çš„æ¨ç†çŠ¶æ€
    if model is not None and hasattr(model, 'cleanup_inference'):
        model.cleanup_inference()
    
    # å¼ºåˆ¶åƒåœ¾å›æ”¶
    gc.collect()
    
    # è·å–å½“å‰æ˜¾å­˜çŠ¶æ€
    import torch
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        allocated = torch.cuda.memory_allocated() / 1024**3
        reserved = torch.cuda.memory_reserved() / 1024**3
        print(f"[Clear Cache] Sessions cleaned: {cleaned_sessions}, GPU memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved")
        return {
            "status": "ok",
            "sessions_cleaned": cleaned_sessions,
            "gpu_memory_allocated_gb": round(allocated, 2),
            "gpu_memory_reserved_gb": round(reserved, 2)
        }
    
    return {
        "status": "ok",
        "sessions_cleaned": cleaned_sessions
    }


# ============================================================
# RESTful APIï¼ˆHTTP è½®è¯¢æ¨¡å¼ï¼Œæ›¿ä»£ WebSocketï¼‰
# ============================================================
import base64
from fastapi import HTTPException
from fastapi.responses import Response
from pydantic import BaseModel
from typing import Optional, List
import uuid
import threading
import time

# å­˜å‚¨æ´»è·ƒçš„ç”Ÿæˆä¼šè¯
active_sessions = {}
session_lock = threading.Lock()

# å…¨å±€æ¨ç†é” - ç¡®ä¿åŒä¸€æ—¶é—´åªæœ‰ä¸€ä¸ªè¯·æ±‚åœ¨ä½¿ç”¨æ¨¡å‹
# è¿™æ˜¯å¿…è¦çš„ï¼Œå› ä¸ºæ‰€æœ‰ session å…±äº«åŒä¸€ä¸ª model å®ä¾‹
inference_lock = threading.Lock()

# SESSION_TIMEOUT ä» config.py å¯¼å…¥

class StartGenerationRequest(BaseModel):
    prompt: str
    num_blocks: int = 25
    num_denoising_steps: int = 4
    strength: float = 0.45
    seed: Optional[int] = None
    start_frame: Optional[str] = None  # base64 encoded

class FrameRequest(BaseModel):
    session_id: str
    image: Optional[str] = None  # base64 encoded
    prompt: Optional[str] = None
    strength: Optional[float] = None

class GenerationSession:
    def __init__(self, session_id: str, model_instance):
        self.session_id = session_id
        self.model = model_instance
        self.initialized = False
        self.current_block = 0
        self.num_blocks = 25
        self.pending_frames = []  # å¾…å‘é€çš„å¸§
        self.lock = threading.Lock()
        self.last_activity = time.time()  # æœ€åæ´»åŠ¨æ—¶é—´
        self.is_generating = False  # æ ‡è®°å½“å‰æ˜¯å¦æ­£åœ¨ç”Ÿæˆ
        
        # æ¯ä¸ª session ç‹¬ç«‹çš„ stateï¼ˆé¿å…å…±äº« model.state å¯¼è‡´çš„å†²çªï¼‰
        self.state = None
        self.prompt = ""
        self.strength = 0.45
        self.generator = None
        self.block_idx = 0
        
        # å¸§ç¼“å­˜ï¼šstreaming æ¨¡å¼éœ€è¦ç¼“å­˜å¤šå¸§å†å¤„ç†
        self.input_frame_buffer = []
        self.frames_per_chunk = FRAMES_PER_CHUNK  # ä» config.py å¯¼å…¥
        self.start_frame = None  # ä¿å­˜èµ·å§‹å¸§
    
    def touch(self):
        """æ›´æ–°æœ€åæ´»åŠ¨æ—¶é—´"""
        self.last_activity = time.time()
    
    def is_expired(self):
        """æ£€æŸ¥ session æ˜¯å¦è¶…æ—¶"""
        return time.time() - self.last_activity > SESSION_TIMEOUT

@app.post("/api/generate/start")
async def api_start_generation(req: StartGenerationRequest):
    """å¼€å§‹ç”Ÿæˆä¼šè¯"""
    if model is None:
        raise HTTPException(status_code=503, detail="Model not loaded yet")
    
    session_id = str(uuid.uuid4())[:8]
    
    # åˆ›å»ºä¼šè¯
    session = GenerationSession(session_id, model)
    session.num_blocks = req.num_blocks
    session.prompt = req.prompt
    session.strength = req.strength
    
    # å¤„ç†èµ·å§‹å¸§
    start_frame = None
    start_frames_list = None  # ç”¨äº V2V æ¨¡å¼çš„å¸§åˆ—è¡¨
    if req.start_frame:
        start_frame_bytes = base64.b64decode(req.start_frame)
        start_frame = model.process_frame_bytes(start_frame_bytes)
        session.start_frame = start_frame  # ä¿å­˜åˆ° session
        
        # V2V æ¨¡å¼ï¼šå¤åˆ¶ start_frame æ¥å¡«å……åˆå§‹å¸§ç¼“å­˜
        # è¿™æ · VAE ç¼–ç æ—¶æœ‰è¶³å¤Ÿçš„å¸§
        start_frames_list = [start_frame] * V2V_INITIAL_FRAMES
    
    # ä½¿ç”¨æ¨ç†é”ç¡®ä¿åŒä¸€æ—¶é—´åªæœ‰ä¸€ä¸ªè¯·æ±‚åœ¨ä½¿ç”¨æ¨¡å‹
    def init_and_generate():
        with inference_lock:
            # ä½¿ç”¨ session ç‹¬ç«‹çš„ state
            state, generator = model.initialize_generation_with_state(
                prompt=req.prompt,
                start_frame=start_frame,
                num_inference_steps=req.num_denoising_steps,
                strength=req.strength,
                seed=req.seed
            )
            session.state = state
            session.generator = generator
            session.block_idx = 0
            
            # ç”Ÿæˆç¬¬ä¸€ä¸ª block
            # å¦‚æœæœ‰ start_frameï¼Œä¼ å…¥å¤åˆ¶çš„å¸§åˆ—è¡¨ä»¥ç¡®ä¿ VAE æœ‰è¶³å¤Ÿå¸§
            new_state, frames = model.generate_next_block_with_state(
                state=session.state,
                prompt=session.prompt,
                strength=session.strength,
                block_idx=session.block_idx,
                generator=session.generator,
                input_frame=start_frames_list,  # ä¼ å…¥å¸§åˆ—è¡¨è€Œä¸æ˜¯ None
                start_frame=None,  # ä¸å†å•ç‹¬ä¼  start_frame
                num_blocks=session.num_blocks
            )
            session.state = new_state
            session.block_idx += 1
            return frames
    
    session.is_generating = True
    try:
        frames = await asyncio.to_thread(init_and_generate)
    finally:
        session.is_generating = False
    
    session.initialized = True
    
    # è½¬æ¢å¸§ä¸º base64
    for frame in frames:
        frame_bytes = model.frame_to_bytes(frame)
        session.pending_frames.append(base64.b64encode(frame_bytes).decode())
    
    session.current_block = 1
    
    with session_lock:
        active_sessions[session_id] = session
    
    print(f"[HTTP] Session {session_id} started, generated block 0")
    
    return {
        "session_id": session_id,
        "status": "started",
        "frames_ready": len(session.pending_frames)
    }

@app.post("/api/generate/frame")
async def api_generate_frame(req: FrameRequest):
    """å‘é€å¸§å¹¶è·å–ç”Ÿæˆçš„å¸§"""
    with session_lock:
        session = active_sessions.get(req.session_id)
    
    if not session:
        raise HTTPException(status_code=404, detail="Session not found")
    
    # æ›´æ–°æ´»åŠ¨æ—¶é—´
    session.touch()
    
    # å¦‚æœæ­£åœ¨ç”Ÿæˆä¸­ï¼Œç›´æ¥è¿”å›å½“å‰çŠ¶æ€ï¼ˆé¿å… DDOSï¼‰
    if session.is_generating:
        with session.lock:
            frames_to_send = session.pending_frames[:5]
            session.pending_frames = session.pending_frames[5:]
        return {
            "session_id": req.session_id,
            "current_block": session.current_block,
            "total_blocks": session.num_blocks,
            "frames": frames_to_send,
            "complete": False,
            "generating": True  # å‘Šè¯‰å‰ç«¯æ­£åœ¨ç”Ÿæˆä¸­
        }
    
    # ç”Ÿæˆä¸‹ä¸€ä¸ª block
    if session.current_block < session.num_blocks:
        # æ›´æ–° session å‚æ•°
        if req.prompt:
            session.prompt = req.prompt
        if req.strength:
            session.strength = req.strength
        
        # å¤„ç†è¾“å…¥å¸§ï¼ˆV2V æ¨¡å¼ï¼‰
        input_frames_for_generation = None
        should_generate = True
        
        if req.image:
            input_frame_bytes = base64.b64decode(req.image)
            input_frame = session.model.process_frame_bytes(input_frame_bytes)
            
            # æ·»åŠ åˆ°å¸§ç¼“å­˜
            session.input_frame_buffer.append(input_frame)
            
            # Streaming V2V ç­–ç•¥ï¼š
            # - ç¬¬ä¸€æ¬¡ V2V ç”Ÿæˆï¼ˆblock_idx=0 æœ‰ start_frameï¼Œæˆ–ç¬¬ä¸€æ¬¡æœ‰ input_frameï¼‰ï¼šéœ€è¦è¾ƒå¤šå¸§
            # - åç»­ç”Ÿæˆï¼špipeline å†…éƒ¨çš„ input_frames_cache å·²ç»æœ‰å¸§äº†ï¼Œåªéœ€è¦å°‘é‡æ–°å¸§
            #
            # KREA çš„ input_frames_cache æ˜¯ deque(maxlen=24)ï¼Œä¼šç´¯ç§¯å¸§
            
            is_first_v2v = (session.block_idx <= 1)  # å‰ä¸¤ä¸ª block éœ€è¦æ›´å¤šå¸§å»ºç«‹ç¼“å­˜
            min_frames_needed = V2V_INITIAL_FRAMES if is_first_v2v else V2V_SUBSEQUENT_FRAMES
            
            if len(session.input_frame_buffer) >= min_frames_needed:
                # æœ‰è¶³å¤Ÿå¸§ï¼Œä¼ å…¥ç¼“å­˜çš„å¸§
                input_frames_for_generation = session.input_frame_buffer.copy()
                # æ¸…ç©ºç¼“å­˜ï¼ˆpipeline å†…éƒ¨ä¼šä¿ç•™å¸§ï¼‰
                session.input_frame_buffer = []
            else:
                # å¸§ä¸å¤Ÿï¼Œè·³è¿‡ç”Ÿæˆï¼Œç­‰å¾…æ›´å¤šå¸§
                should_generate = False
                if is_first_v2v:
                    print(f"[HTTP] Session {req.session_id}: buffering frames {len(session.input_frame_buffer)}/{min_frames_needed}")
        
        # T2V æ¨¡å¼ï¼šä¸éœ€è¦è¾“å…¥å¸§ï¼Œç›´æ¥ç”Ÿæˆ
        # V2V æ¨¡å¼ï¼šéœ€è¦è¶³å¤Ÿçš„å¸§æ‰ç”Ÿæˆ
        
        if should_generate:
            # ä½¿ç”¨æ¨ç†é”ç¡®ä¿åŒä¸€æ—¶é—´åªæœ‰ä¸€ä¸ªè¯·æ±‚åœ¨ä½¿ç”¨æ¨¡å‹
            def generate_with_lock():
                with inference_lock:
                    new_state, frames = session.model.generate_next_block_with_state(
                        state=session.state,
                        prompt=session.prompt,
                        strength=session.strength,
                        block_idx=session.block_idx,
                        generator=session.generator,
                        input_frame=input_frames_for_generation if input_frames_for_generation else None,
                        start_frame=session.start_frame if session.block_idx == 0 else None,
                        num_blocks=session.num_blocks
                    )
                    session.state = new_state
                    session.block_idx += 1
                    return frames
            
            session.is_generating = True
            try:
                frames = await asyncio.to_thread(generate_with_lock)
                
                with session.lock:
                    for frame in frames:
                        frame_bytes = session.model.frame_to_bytes(frame)
                        session.pending_frames.append(base64.b64encode(frame_bytes).decode())
                
                session.current_block += 1
            except Exception as e:
                print(f"[HTTP] Session {req.session_id} generation error: {e}")
                import traceback
                traceback.print_exc()
            finally:
                session.is_generating = False
    
    # è¿”å›å¾…å‘é€çš„å¸§
    with session.lock:
        frames_to_send = session.pending_frames[:5]  # æ¯æ¬¡æœ€å¤šè¿”å›5å¸§
        session.pending_frames = session.pending_frames[5:]
    
    return {
        "session_id": req.session_id,
        "current_block": session.current_block,
        "total_blocks": session.num_blocks,
        "frames": frames_to_send,
        "complete": session.current_block >= session.num_blocks and len(session.pending_frames) == 0
    }

@app.get("/api/generate/frames/{session_id}")
async def api_get_frames(session_id: str, count: int = 5):
    """è·å–ç”Ÿæˆçš„å¸§ï¼ˆä¸å‘é€æ–°è¾“å…¥ï¼‰"""
    with session_lock:
        session = active_sessions.get(session_id)
    
    if not session:
        raise HTTPException(status_code=404, detail="Session not found")
    
    # æ›´æ–°æ´»åŠ¨æ—¶é—´
    session.touch()
    
    with session.lock:
        frames_to_send = session.pending_frames[:count]
        session.pending_frames = session.pending_frames[count:]
    
    return {
        "session_id": session_id,
        "frames": frames_to_send,
        "frames_remaining": len(session.pending_frames),
        "complete": session.current_block >= session.num_blocks and len(session.pending_frames) == 0
    }

@app.post("/api/generate/stop/{session_id}")
async def api_stop_generation(session_id: str):
    """åœæ­¢ç”Ÿæˆä¼šè¯"""
    with session_lock:
        if session_id in active_sessions:
            session = active_sessions[session_id]
            # æ¸…ç†æ¨ç†æ˜¾å­˜
            if hasattr(session.model, 'cleanup_inference'):
                session.model.cleanup_inference()
            del active_sessions[session_id]
            print(f"[HTTP] Session {session_id} stopped, memory cleaned")
            return {"status": "stopped"}
    
    return {"status": "not_found"}


@app.websocket("/ws/video-gen")
async def websocket_video_gen(websocket: WebSocket):
    """WebSocket å¤„ç† - ä½¿ç”¨æœ¬åœ° GPU"""
    await websocket.accept()
    
    active_websockets.add(websocket)
    print(f"WebSocket connected. Active connections: {len(active_websockets)}")
    
    try:
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½
        if model is None:
            await websocket.close(code=1011, reason="Model not loaded yet, please wait...")
            return
        
        inference_model = model
        
        # å‘é€ ready ä¿¡å·
        await websocket.send_text(json.dumps({"status": "ready"}))
        print("Sent ready signal to client")
        
        # åˆå§‹åŒ–æ ‡å¿—
        initialized = False
        prompt = ""
        num_blocks = 25
        current_block = 0
        
        while True:
            # æ¥æ”¶æ¶ˆæ¯
            data = await websocket.receive_bytes()
            
            # è§£æ msgpack
            message = msgpack.unpackb(data, raw=False)
            
            # åˆå§‹åŒ–å‚æ•°
            if not initialized and "prompt" in message:
                prompt = message.get("prompt", "")
                num_blocks = message.get("num_blocks", 25)
                num_inference_steps = message.get("num_denoising_steps", 4)
                strength = message.get("strength", 0.45)
                seed = message.get("seed")
                start_frame = message.get("start_frame")  # å¯èƒ½æ˜¯ bytes
                
                print(f"Initializing: prompt='{prompt}', num_blocks={num_blocks}")
                
                # ä½¿ç”¨æ¨ç†é”åˆå§‹åŒ–ç”Ÿæˆå¹¶ç”Ÿæˆç¬¬ä¸€ä¸ª block
                def init_and_generate_first():
                    with inference_lock:
                        inference_model.initialize_generation(
                            prompt=prompt,
                            start_frame=start_frame,
                            num_inference_steps=num_inference_steps,
                            strength=strength,
                            seed=seed
                        )
                        print(f"Generating block 0/{num_blocks}")
                        return inference_model.generate_next_block(input_frame=None)
                
                frames = await asyncio.to_thread(init_and_generate_first)
                
                initialized = True
                current_block = 0
                
                # å‘é€å¸§
                for frame in frames:
                    frame_bytes = inference_model.frame_to_bytes(frame)
                    await websocket.send_bytes(frame_bytes)
                
                current_block += 1
                
            # æ›´æ–°å‚æ•°ï¼ˆprompt æˆ– num_blocks å˜åŒ–ï¼‰
            elif initialized and "prompt" in message and "image" not in message:
                new_prompt = message.get("prompt")
                new_num_blocks = message.get("num_blocks")
                
                if new_prompt != prompt:
                    print(f"Prompt updated: '{new_prompt}'")
                    prompt = new_prompt
                    inference_model.prompt = prompt
                    
                if new_num_blocks != num_blocks:
                    print(f"num_blocks updated: {new_num_blocks}")
                    num_blocks = new_num_blocks
            
            # æ¥æ”¶è¾“å…¥å¸§ï¼ˆvideo-to-video æˆ– webcam æ¨¡å¼ï¼‰
            elif initialized and "image" in message:
                input_frame_bytes = message["image"]
                strength = message.get("strength", 0.45)
                
                # æ›´æ–° num_blocksï¼ˆè¿™ä¸ªä¸éœ€è¦é”ï¼‰
                if "num_blocks" in message:
                    num_blocks = message["num_blocks"]
                
                # ç”Ÿæˆä¸‹ä¸€ä¸ª block
                if current_block < num_blocks:
                    input_frame = inference_model.process_frame_bytes(input_frame_bytes)
                    
                    # ä½¿ç”¨æ¨ç†é”
                    def generate_with_lock():
                        with inference_lock:
                            inference_model.strength = strength
                            if "prompt" in message:
                                inference_model.prompt = message["prompt"]
                            return inference_model.generate_next_block(input_frame=input_frame)
                    
                    frames = await asyncio.to_thread(generate_with_lock)
                    
                    # å‘é€å¸§
                    for frame in frames:
                        frame_bytes = inference_model.frame_to_bytes(frame)
                        await websocket.send_bytes(frame_bytes)
                    
                    current_block += 1
                    
                    if current_block >= num_blocks:
                        print(f"Generation complete: {current_block} blocks")
                        
    except Exception as e:
        print(f"WebSocket error: {e}")
        import traceback
        traceback.print_exc()
    finally:
        active_websockets.discard(websocket)
        # æ¸…ç†æ¨ç†æ˜¾å­˜
        if model is not None and hasattr(model, 'cleanup_inference'):
            model.cleanup_inference()
            print("Inference memory cleaned")
        print(f"WebSocket disconnected. Active connections: {len(active_websockets)}")


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=7860)
