"""
FP8 ä¼˜åŒ–æ¨¡å— - åŸºäºŽ ComfyUI-WanVideoWrapper çš„å®žçŽ°
åªå¯¹ Linear å±‚åº”ç”¨ FP8 è®¡ç®—ï¼Œä¿æŒ Conv/Norm ç­‰å±‚ä¸ºåŽŸç²¾åº¦
"""
import torch
import torch.nn as nn


def fp8_linear_forward(cls, base_dtype, input):
    """FP8 Linear å±‚çš„ forward å‡½æ•°
    
    åŸºäºŽ ComfyUI å’Œ MinusZoneAI çš„ fp8_linear ä¼˜åŒ–
    ä½¿ç”¨ torch._scaled_mm è¿›è¡Œ FP8 çŸ©é˜µä¹˜æ³•
    """
    weight_dtype = cls.weight.dtype
    
    if weight_dtype in [torch.float8_e4m3fn, torch.float8_e5m2]:
        if len(input.shape) == 3:
            input_shape = input.shape

            # èŽ·å–æˆ–åˆ›å»º scale_weight
            scale_weight = getattr(cls, 'scale_weight', None)
            if scale_weight is None:
                scale_weight = torch.ones((), device=input.device, dtype=torch.float32)
            else:
                scale_weight = scale_weight.to(input.device).squeeze()

            scale_input = torch.ones((), device=input.device, dtype=torch.float32)

            # Clamp è¾“å…¥åˆ° FP8 e4m3fn çš„æœ‰æ•ˆèŒƒå›´
            input = torch.clamp(input, min=-448, max=448, out=input)
            
            # è½¬æ¢è¾“å…¥ä¸º FP8ï¼ˆå§‹ç»ˆä½¿ç”¨ e4m3fnï¼Œå› ä¸º e5m2 * e5m2 ä¸æ”¯æŒï¼‰
            inn = input.reshape(-1, input_shape[2]).to(torch.float8_e4m3fn).contiguous()

            bias = cls.bias.to(base_dtype) if cls.bias is not None else None

            # ä½¿ç”¨ scaled_mm è¿›è¡Œ FP8 çŸ©é˜µä¹˜æ³•
            o = torch._scaled_mm(
                inn, 
                cls.weight.t(), 
                out_dtype=base_dtype, 
                bias=bias, 
                scale_a=scale_input, 
                scale_b=scale_weight
            )

            return o.reshape((-1, input_shape[1], cls.weight.shape[0]))
        else:
            # éž 3D è¾“å…¥ï¼Œå›žé€€åˆ°åŽŸå§‹ forward
            return cls.original_forward(input.to(base_dtype))
    else:
        return cls.original_forward(input)


def convert_fp8_linear(module, base_dtype, params_to_keep=None, scale_weight_keys=None):
    """å°†æ¨¡åž‹ä¸­çš„ Linear å±‚è½¬æ¢ä¸º FP8 ä¼˜åŒ–ç‰ˆæœ¬
    
    Args:
        module: è¦è½¬æ¢çš„æ¨¡åž‹
        base_dtype: åŸºç¡€æ•°æ®ç±»åž‹ï¼ˆè¾“å‡ºç±»åž‹ï¼‰
        params_to_keep: éœ€è¦ä¿æŒåŽŸç²¾åº¦çš„å‚æ•°åå…³é”®å­—é›†åˆ
        scale_weight_keys: scale_weight å­—å…¸
    """
    if params_to_keep is None:
        # è¿™äº›å±‚ä¿æŒåŽŸç²¾åº¦ï¼Œä¸åš FP8 é‡åŒ–
        params_to_keep = {
            "norm", "bias", "time_in", "patch_embedding", "time_", 
            "img_emb", "modulation", "text_embedding", "adapter", 
            "add", "ref_conv", "audio_proj"
        }
    
    print("ðŸ”§ å¯ç”¨ FP8 çŸ©é˜µä¹˜æ³•ä¼˜åŒ–...")
    converted_count = 0
    skipped_count = 0
    
    for name, submodule in module.named_modules():
        # è·³è¿‡éœ€è¦ä¿æŒåŽŸç²¾åº¦çš„å±‚
        if any(keyword in name for keyword in params_to_keep):
            skipped_count += 1
            continue
            
        if isinstance(submodule, nn.Linear):
            # æ£€æŸ¥æ˜¯å¦æœ‰ scale_weight
            if scale_weight_keys is not None:
                scale_key = f"{name}.scale_weight"
                if scale_key in scale_weight_keys:
                    setattr(submodule, "scale_weight", scale_weight_keys[scale_key].float())
            
            # ä¿å­˜åŽŸå§‹ forward å¹¶æ›¿æ¢ä¸º FP8 ç‰ˆæœ¬
            original_forward = submodule.forward
            setattr(submodule, "original_forward", original_forward)
            setattr(submodule, "forward", 
                    lambda input, m=submodule: fp8_linear_forward(m, base_dtype, input))
            converted_count += 1
    
    print(f"   âœ… å·²è½¬æ¢ {converted_count} ä¸ª Linear å±‚ä¸º FP8")
    print(f"   â­ï¸  è·³è¿‡ {skipped_count} ä¸ªç‰¹æ®Šå±‚ï¼ˆä¿æŒåŽŸç²¾åº¦ï¼‰")


def load_fp8_weights(model, state_dict, base_dtype=torch.bfloat16, device="cuda"):
    """åŠ è½½ FP8 æƒé‡åˆ°æ¨¡åž‹
    
    Args:
        model: æ¨¡åž‹å®žä¾‹
        state_dict: FP8 æƒé‡å­—å…¸
        base_dtype: éž FP8 å±‚çš„æ•°æ®ç±»åž‹
        device: ç›®æ ‡è®¾å¤‡
    
    Returns:
        scale_weights: scale_weight å­—å…¸
    """
    # éœ€è¦ä¿æŒåŽŸç²¾åº¦çš„å‚æ•°
    params_to_keep = {
        "norm", "bias", "time_in", "patch_embedding", "time_", 
        "img_emb", "modulation", "text_embedding", "adapter", 
        "add", "ref_conv", "audio_proj"
    }
    
    # æå– scale_weights
    scale_weights = {}
    for k, v in state_dict.items():
        if k.endswith(".scale_weight") or k.endswith(".weight_scale"):
            scale_weights[k.replace(".weight_scale", ".scale_weight")] = v.to(device, torch.float32)
    
    print(f"   æ‰¾åˆ° {len(scale_weights)} ä¸ª scale_weight")
    
    # åŠ è½½æƒé‡
    model_state = model.state_dict()
    loaded_count = 0
    
    for name, param in state_dict.items():
        if name in model_state:
            # åˆ¤æ–­æ˜¯å¦éœ€è¦ä¿æŒåŽŸç²¾åº¦
            keep_original = any(keyword in name for keyword in params_to_keep)
            
            if keep_original or not isinstance(param, torch.Tensor):
                # è½¬æ¢ä¸º base_dtype
                if isinstance(param, torch.Tensor):
                    param = param.to(device, base_dtype)
            else:
                # FP8 æƒé‡ä¿æŒåŽŸæ ·
                param = param.to(device)
            
            try:
                model_state[name].copy_(param)
                loaded_count += 1
            except Exception as e:
                print(f"   âš ï¸  æ— æ³•åŠ è½½ {name}: {e}")
    
    print(f"   âœ… å·²åŠ è½½ {loaded_count} ä¸ªå‚æ•°")
    
    return scale_weights


def check_fp8_support():
    """æ£€æŸ¥å½“å‰ç¡¬ä»¶æ˜¯å¦æ”¯æŒ FP8 è®¡ç®—
    
    Returns:
        tuple: (supports_fp8, compute_capability, message)
    """
    if not torch.cuda.is_available():
        return False, None, "CUDA ä¸å¯ç”¨"
    
    major, minor = torch.cuda.get_device_capability()
    compute_cap = f"{major}.{minor}"
    
    # FP8 matmul éœ€è¦ CUDA compute capability >= 8.9 (RTX 4000 ç³»åˆ—åŠä»¥ä¸Š)
    if (major, minor) >= (8, 9):
        return True, compute_cap, f"å®Œå…¨æ”¯æŒ FP8 (Compute Capability {compute_cap})"
    elif (major, minor) >= (8, 0):
        return True, compute_cap, f"éƒ¨åˆ†æ”¯æŒ FP8 (Compute Capability {compute_cap})ï¼ŒæŽ¨è RTX 4000+ ç³»åˆ—"
    else:
        return False, compute_cap, f"ä¸æ”¯æŒ FP8 (Compute Capability {compute_cap})ï¼Œéœ€è¦ >= 8.0"
