"""
BF16 æ ‡å‡†åŠ è½½ï¼ˆæ— é‡åŒ–ï¼‰
éœ€è¦ ~54GB+ æ˜¾å­˜
"""
import torch


def load_bf16(pipe, repo_id, device, dtype):
    """BF16 æ ‡å‡†åŠ è½½æ¨¡å‹ï¼ˆæ— é‡åŒ–ï¼‰
    
    Args:
        pipe: ModularPipeline å®ä¾‹
        repo_id: æ¨¡å‹ä»“åº“ ID
        device: ç›®æ ‡è®¾å¤‡
        dtype: æ•°æ®ç±»å‹ (é»˜è®¤ bfloat16)
    
    Returns:
        åŠ è½½å®Œæˆçš„ pipe
    """
    print("ğŸ”§ BF16 æ ‡å‡†åŠ è½½ï¼ˆæ— é‡åŒ–ï¼‰...")
    print("   âš ï¸  éœ€è¦ ~54GB+ æ˜¾å­˜")
    
    # CUDA æ€§èƒ½ä¼˜åŒ–
    torch.backends.cudnn.benchmark = True
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    print("   âœ… CUDA ä¼˜åŒ–å·²å¯ç”¨ (cudnn.benchmark, TF32)")
    
    pipe.load_components(
        trust_remote_code=True,
        device_map=device,
        torch_dtype={"default": dtype, "vae": torch.float16},
    )
    
    # èåˆæŠ•å½±å±‚ä¼˜åŒ–
    print("ğŸ”§ èåˆæŠ•å½±å±‚...")
    for block in pipe.transformer.blocks:
        block.self_attn.fuse_projections()
    
    print("   âœ… BF16 åŠ è½½å®Œæˆ")
    return pipe
