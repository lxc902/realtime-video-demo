"""
BF16 æ ‡å‡†åŠ è½½ï¼ˆæ— é‡åŒ–ï¼‰
éœ€è¦ ~54GB+ æ˜¾å­˜
"""
import torch
import os


def load_bf16(pipe, repo_id, device, dtype):
    """BF16 æ ‡å‡†åŠ è½½æ¨¡å‹ï¼ˆæ— é‡åŒ–ï¼‰
    
    Args:
        pipe: ModularPipeline å®ä¾‹
        repo_id: æ¨¡å‹ä»“åº“ ID
        device: ç›®æ ‡è®¾å¤‡
        dtype: æ•°æ®ç±»å‹ (é»˜è®¤ bfloat16)
    
    Returns:
        åŠ è½½å®Œæˆçš„ pipe
    """
    print("ğŸ”§ BF16 æ ‡å‡†åŠ è½½ï¼ˆæ— é‡åŒ–ï¼‰...")
    print("   âš ï¸  éœ€è¦ ~54GB+ æ˜¾å­˜")
    
    # CUDA æ€§èƒ½ä¼˜åŒ–
    torch.backends.cudnn.benchmark = True
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    print("   âœ… CUDA ä¼˜åŒ–å·²å¯ç”¨ (cudnn.benchmark, TF32)")
    
    pipe.load_components(
        trust_remote_code=True,
        device_map=device,
        torch_dtype={"default": dtype, "vae": torch.float16},
    )
    
    # èåˆæŠ•å½±å±‚ä¼˜åŒ–
    print("ğŸ”§ èåˆæŠ•å½±å±‚...")
    for block in pipe.transformer.blocks:
        block.self_attn.fuse_projections()
    
    # torch.compile ä¼˜åŒ–ï¼ˆå¤§æ˜¾å­˜æ¨èï¼‰
    if os.environ.get("DISABLE_COMPILE", "0") != "1":
        try:
            print("ğŸ”§ ç¼–è¯‘ transformer (torch.compile)...")
            pipe.transformer = torch.compile(
                pipe.transformer,
                mode="reduce-overhead",  # å‡å°‘ kernel launch å¼€é”€
                fullgraph=False,
            )
            print("   âœ… torch.compile å®Œæˆ")
        except Exception as e:
            print(f"   âš ï¸  torch.compile è·³è¿‡: {e}")
    
    print("   âœ… BF16 åŠ è½½å®Œæˆ")
    return pipe
