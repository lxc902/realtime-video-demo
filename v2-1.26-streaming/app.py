"""
KREA Realtime Video v2 - Streaming ç‰ˆæœ¬
ç®€åŒ–çš„åç«¯ï¼Œä¸“æ³¨äº SSE æµå¼ç”Ÿæˆ
"""
import os
import asyncio
import json
import base64
import uuid
import threading
import time
from datetime import datetime, timezone, timedelta

# åŒ—äº¬æ—¶é—´
def beijing_time():
    return datetime.now(timezone(timedelta(hours=8))).strftime("%H:%M:%S.%f")[:-3]
import gc
from typing import Optional

from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import HTMLResponse, StreamingResponse
from fastapi.templating import Jinja2Templates
from fastapi.middleware.cors import CORSMiddleware
from starlette.middleware.base import BaseHTTPMiddleware
from pydantic import BaseModel

# å¯¼å…¥æœ¬åœ°æ¨ç†æ¨¡å—
from local_inference import get_model
from config import (
    MODEL_PATH, QUANTIZATION,
    NUM_INFERENCE_STEPS, DEFAULT_STRENGTH,
    V2V_INITIAL_FRAMES
)

app = FastAPI(title="KREA Realtime Video v2")
templates = Jinja2Templates(directory=".")

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# CSP Middleware
class CSPMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        response = await call_next(request)
        if "text/html" in response.headers.get("content-type", ""):
            response.headers["Content-Security-Policy"] = (
                "default-src 'self'; "
                "script-src 'self' 'unsafe-inline' 'unsafe-eval' https://unpkg.com; "
                "style-src 'self' 'unsafe-inline' https://fonts.googleapis.com; "
                "font-src 'self' https://fonts.gstatic.com; "
                "connect-src 'self' wss: https:; "
                "img-src 'self' data: blob:; "
                "media-src 'self' blob:;"
            )
        return response

app.add_middleware(CSPMiddleware)

# å…¨å±€æ¨¡å‹å®ä¾‹
model = None

# æ¨ç†é” - ç¡®ä¿åŒä¸€æ—¶é—´åªæœ‰ä¸€ä¸ªè¯·æ±‚ä½¿ç”¨æ¨¡å‹
inference_lock = threading.Lock()


def load_model_on_startup():
    """å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹"""
    global model
    print("")
    print("=" * 60)
    print("ğŸ”¥ Loading KREA model to GPU...")
    if MODEL_PATH:
        print(f"   From: {MODEL_PATH}")
    else:
        print("   From: HuggingFace (krea/krea-realtime-video)")
    if QUANTIZATION:
        print(f"   Quantization: {QUANTIZATION.upper()}")
    else:
        print("   Quantization: None (full precision)")
    print("=" * 60)
    print("")
    model = get_model(model_path=MODEL_PATH, quantization=QUANTIZATION)
    print("")
    print("=" * 60)
    print("âœ… Model loaded successfully!")
    print("ğŸŒ Server ready at http://localhost:7860")
    print("=" * 60)
    print("")


@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨äº‹ä»¶"""
    loop = asyncio.get_event_loop()
    await loop.run_in_executor(None, load_model_on_startup)


@app.get("/", response_class=HTMLResponse)
async def home(request: Request):
    """é¦–é¡µ"""
    return templates.TemplateResponse("index.html", {"request": request})


@app.get("/health")
async def health():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "ok",
        "model_loaded": model is not None,
        "version": "v2-streaming"
    }


# ============================================================
# å®æ—¶å¸§ç¼“å­˜ï¼ˆå‰ç«¯æŒç»­æ›´æ–°ï¼Œåç«¯ç”Ÿæˆæ—¶ä½¿ç”¨æœ€æ–°å¸§ï¼‰
# ============================================================
latest_frame_lock = threading.Lock()
latest_frame_data = {
    "frame": None,  # æœ€æ–°å¸§ (numpy array)
    "timestamp": 0,  # æœåŠ¡å™¨æ—¶é—´
    "client_ts": 0,  # å®¢æˆ·ç«¯æ—¶é—´æˆ³ï¼ˆå‰ç«¯å‘é€ï¼‰
    "strength": None,  # æœ€æ–° strength
    "prompt": None     # æœ€æ–° prompt
}

# ============================================================
# API Models
# ============================================================

class StreamGenerationRequest(BaseModel):
    prompt: str
    num_blocks: int = 5  # è®¾ä¸º 0 è¡¨ç¤ºæ— é™ç”Ÿæˆ
    num_denoising_steps: int = NUM_INFERENCE_STEPS
    strength: float = DEFAULT_STRENGTH
    seed: Optional[int] = None
    start_frame: Optional[str] = None  # base64 encodedï¼ˆé¦–å¸§ï¼Œåç»­ç”¨ update_frameï¼‰

class UpdateFrameRequest(BaseModel):
    frame: Optional[str] = None  # base64 encoded (å¯é€‰)
    timestamp: float = 0  # å®¢æˆ·ç«¯æ—¶é—´æˆ³ï¼ˆmsï¼‰
    strength: Optional[float] = None
    prompt: Optional[str] = None


# ============================================================
# å¸§æ›´æ–° APIï¼ˆå‰ç«¯æŒç»­è°ƒç”¨ï¼‰
# ============================================================

@app.post("/api/update_frame")
async def api_update_frame(req: UpdateFrameRequest):
    """å‰ç«¯æŒç»­å‘é€æœ€æ–°å¸§ï¼Œåç«¯ç¼“å­˜"""
    global latest_frame_data
    try:
        with latest_frame_lock:
            # åªæœ‰ frame å­˜åœ¨æ—¶æ‰æ›´æ–°å¸§æ•°æ®
            if req.frame:
                frame_bytes = base64.b64decode(req.frame)
                frame = model.process_frame_bytes(frame_bytes) if model else None
                latest_frame_data["frame"] = frame
                latest_frame_data["timestamp"] = time.time()
                latest_frame_data["client_ts"] = req.timestamp
            
            # strength/prompt å§‹ç»ˆæ›´æ–°
            if req.strength is not None:
                latest_frame_data["strength"] = req.strength
            if req.prompt is not None:
                latest_frame_data["prompt"] = req.prompt
        
        return {"status": "ok"}
    except Exception as e:
        return {"status": "error", "message": str(e)}


# ============================================================
# SSE æµå¼ç”Ÿæˆ API
# ============================================================

@app.post("/api/generate/stream")
async def api_stream_generation(req: StreamGenerationRequest):
    """SSE æµå¼ç”Ÿæˆ - æ¯ç”Ÿæˆä¸€å¸§ç«‹å³æ¨é€"""
    if model is None:
        raise HTTPException(status_code=503, detail="Model not loaded yet")
    
    async def generate_stream():
        session_id = str(uuid.uuid4())[:8]
        print(f"[{beijing_time()}] SSE session {session_id} started")
        
        try:
            # å¤„ç†èµ·å§‹å¸§
            start_frame = None
            start_frames_list = None
            if req.start_frame:
                start_frame_bytes = base64.b64decode(req.start_frame)
                start_frame = model.process_frame_bytes(start_frame_bytes)
                start_frames_list = [start_frame] * V2V_INITIAL_FRAMES
            
            # åˆå§‹åŒ– state
            state = None
            generator = None
            
            def init_generation():
                nonlocal state, generator
                with inference_lock:
                    state, generator = model.initialize_generation_with_state(
                        prompt=req.prompt,
                        start_frame=start_frame,
                        num_inference_steps=req.num_denoising_steps,
                        strength=req.strength,
                        seed=req.seed
                    )
            
            await asyncio.to_thread(init_generation)
            
            # ç”Ÿæˆå¾ªç¯ï¼ˆnum_blocks=0 è¡¨ç¤ºæ— é™ç”Ÿæˆï¼‰
            block_idx = 0
            max_blocks = req.num_blocks if req.num_blocks > 0 else 999999
            global_start_time = time.time() * 1000  # å…¨å±€èµ·å§‹æ—¶é—´
            cumulative_time = 0  # ç´¯è®¡æ—¶é—´ï¼ˆç”¨äºå¸§æ—¶é—´æˆ³ï¼‰
            
            while block_idx < max_blocks:
                current_block = block_idx  # é—­åŒ…æ•è·
                block_start_time = time.time() * 1000  # ms
                
                def generate_block():
                    nonlocal state, start_frames_list
                    input_client_ts = 0  # è¾“å…¥å¸§çš„å®¢æˆ·ç«¯æ—¶é—´æˆ³
                    with inference_lock:
                        # è·å–æœ€æ–°å¸§ã€strengthã€prompt
                        current_prompt = req.prompt
                        current_strength = req.strength
                        with latest_frame_lock:
                            if latest_frame_data["frame"] is not None:
                                latest_frame = latest_frame_data["frame"]
                                start_frames_list = [latest_frame] * V2V_INITIAL_FRAMES
                                input_client_ts = latest_frame_data["client_ts"]
                            if latest_frame_data["strength"] is not None:
                                current_strength = latest_frame_data["strength"]
                            if latest_frame_data["prompt"] is not None:
                                current_prompt = latest_frame_data["prompt"]
                        
                        # æ¯ä¸ª block éƒ½ä½¿ç”¨æœ€æ–°è¾“å…¥å¸§ï¼ˆå‡å°‘å»¶è¿Ÿæ„Ÿï¼‰
                        input_frames = start_frames_list if start_frames_list else None
                        new_state, frames = model.generate_next_block_with_state(
                            state=state,
                            prompt=current_prompt,
                            strength=current_strength,
                            block_idx=current_block,
                            generator=generator,
                            input_frame=input_frames,
                            start_frame=None,
                            num_blocks=max_blocks
                        )
                        state = new_state
                        return frames, input_client_ts
                
                frames, input_ts = await asyncio.to_thread(generate_block)
                block_end_time = time.time() * 1000  # ms
                
                # æ—¶é—´æ’å€¼ï¼šå°†ç”Ÿæˆè€—æ—¶å‡åŒ€åˆ†é…ç»™æ¯å¸§
                block_duration = block_end_time - block_start_time
                num_frames = len(frames)
                frame_interval = block_duration / num_frames  # æ¯å¸§é—´éš”
                
                for frame_idx, frame in enumerate(frames):
                    frame_bytes = model.frame_to_bytes(frame)
                    frame_b64 = base64.b64encode(frame_bytes).decode()
                    global_frame_idx = block_idx * num_frames + frame_idx + 1
                    
                    # ç›¸å¯¹æ—¶é—´æˆ³ï¼ˆä» 0 å¼€å§‹ï¼Œç´¯åŠ ï¼‰
                    # å‰ç«¯å¯ä»¥ç›´æ¥ç”¨è¿™ä¸ªå·®å€¼æ¥è®¡ç®—æ’­æ”¾é—´éš”
                    frame_ts = cumulative_time + frame_interval * (frame_idx + 1)
                    
                    event_data = json.dumps({
                        "type": "frame",
                        "block": block_idx,
                        "frame_idx": global_frame_idx,
                        "timestamp": frame_ts,
                        "input_ts": input_ts,  # è¾“å…¥å¸§çš„å®¢æˆ·ç«¯æ—¶é—´æˆ³ï¼ˆç”¨äºè®¡ç®—å»¶è¿Ÿï¼‰
                        "data": frame_b64
                    })
                    yield f"data: {event_data}\n\n"
                
                # ç´¯åŠ æ—¶é—´
                cumulative_time += block_duration
                print(f"[{beijing_time()}] Block {block_idx}: {num_frames} frames, {block_duration:.0f}ms")
                block_idx += 1
            
            # å®Œæˆï¼ˆä»…å½“ num_blocks > 0 æ—¶ï¼‰
            if req.num_blocks > 0:
                yield f"data: {json.dumps({'type': 'complete'})}\n\n"
                print(f"[{beijing_time()}] SSE session {session_id} complete")
            
            # æ¸…ç†
            if model is not None and hasattr(model, 'cleanup_inference'):
                model.cleanup_inference()
            gc.collect()
            
        except Exception as e:
            import traceback
            traceback.print_exc()
            yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"
    
    return StreamingResponse(
        generate_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )


@app.post("/api/clear-cache")
async def clear_cache():
    """æ¸…ç†æ¨ç†ç¼“å­˜"""
    if model is not None and hasattr(model, 'cleanup_inference'):
        model.cleanup_inference()
    gc.collect()
    
    import torch
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        allocated = torch.cuda.memory_allocated() / 1024**3
        return {
            "status": "ok",
            "gpu_memory_gb": round(allocated, 2)
        }
    
    return {"status": "ok"}


if __name__ == "__main__":
    import uvicorn
    import signal
    import sys
    
    # Ctrl+C ç«‹å³é€€å‡ºï¼Œä¸ç­‰å¾…è¿æ¥å…³é—­
    def force_exit(sig, frame):
        print("\nå¼ºåˆ¶é€€å‡º...")
        sys.exit(0)
    
    signal.signal(signal.SIGINT, force_exit)
    signal.signal(signal.SIGTERM, force_exit)
    
    uvicorn.run(app, host="0.0.0.0", port=7860)
