"""
KREA Realtime Video - æœ¬åœ° GPU æ¨ç†æ¨¡å—
ä½¿ç”¨ diffusers åº“åœ¨æœ¬åœ° GPU ä¸Šè¿è¡Œ KREA æ¨¡å‹
"""
import torch
from diffusers import ModularPipeline
from diffusers.modular_pipelines import PipelineState
from diffusers.utils import load_video, export_to_video
from PIL import Image
import numpy as np
import io

class KreaLocalInference:
    def __init__(self, device="cuda", dtype=torch.bfloat16, model_path=None, quantization=None):
        """åˆå§‹åŒ–æœ¬åœ° KREA æ¨¡å‹
        
        Args:
            device: è®¾å¤‡ (cuda/cpu)
            dtype: æ•°æ®ç±»å‹
            model_path: è‡ªå®šä¹‰æ¨¡å‹è·¯å¾„ï¼Œå¯ä»¥æ˜¯ï¼š
                       - æœ¬åœ°è·¯å¾„: "/path/to/model"
                       - HuggingFace repo: "krea/krea-realtime-video"
                       - None: ä½¿ç”¨é»˜è®¤ HuggingFace repo
            quantization: é‡åŒ–ç±»å‹ (None, "int8", "int4")
        """
        print("æ­£åœ¨åŠ è½½ KREA Realtime Video æ¨¡å‹...")
        self.device = device
        self.dtype = dtype
        self.quantization = quantization
        
        # ç¡®å®šæ¨¡å‹è·¯å¾„
        if model_path is None:
            # é»˜è®¤ä½¿ç”¨ HuggingFace
            repo_id = "krea/krea-realtime-video"
            print(f"ä» HuggingFace åŠ è½½: {repo_id}")
        else:
            # ä½¿ç”¨è‡ªå®šä¹‰è·¯å¾„
            repo_id = model_path
            print(f"ä»è‡ªå®šä¹‰è·¯å¾„åŠ è½½: {model_path}")
        
        # å‡†å¤‡é‡åŒ–é…ç½®
        quantization_config = None
        if quantization:
            print(f"ğŸ”§ å¯ç”¨ {quantization.upper()} é‡åŒ–...")
            try:
                from transformers import BitsAndBytesConfig
                if quantization == "int8":
                    quantization_config = BitsAndBytesConfig(
                        load_in_8bit=True,
                    )
                    print("   ä½¿ç”¨ 8-bit é‡åŒ– (é¢„è®¡æ˜¾å­˜ ~24GB)")
                elif quantization == "int4":
                    quantization_config = BitsAndBytesConfig(
                        load_in_4bit=True,
                        bnb_4bit_compute_dtype=torch.bfloat16,
                        bnb_4bit_quant_type="nf4",
                    )
                    print("   ä½¿ç”¨ 4-bit é‡åŒ– (é¢„è®¡æ˜¾å­˜ ~12GB)")
            except ImportError:
                print("   âŒ é‡åŒ–éœ€è¦ bitsandbytes åº“")
                print("   è¯·è¿è¡Œ: pip install bitsandbytes")
                quantization_config = None
        
        # åŠ è½½æ¨¡å‹
        self.pipe = ModularPipeline.from_pretrained(repo_id, trust_remote_code=True)
        
        # åŠ è½½ç»„ä»¶ï¼ˆå¸¦é‡åŒ–é…ç½®ï¼‰
        load_kwargs = {
            "trust_remote_code": True,
            "device_map": device,
            "torch_dtype": {"default": dtype, "vae": torch.float16},
        }
        
        if quantization_config:
            load_kwargs["quantization_config"] = quantization_config
        
        self.pipe.load_components(**load_kwargs)
        
        # æ£€æŸ¥å…³é”®ç»„ä»¶æ˜¯å¦åŠ è½½æˆåŠŸ
        if not hasattr(self.pipe, 'transformer') or self.pipe.transformer is None:
            raise RuntimeError(
                "âŒ æ¨¡å‹åŠ è½½å¤±è´¥: transformer ç»„ä»¶æœªæ­£ç¡®åŠ è½½\n"
                "å¯èƒ½åŸå› :\n"
                "1. ç¼ºå°‘ä¾èµ–åŒ…ï¼ˆeinops, imageio, ftfyï¼‰\n"
                "2. diffusers ç‰ˆæœ¬ä¸å…¼å®¹\n"
                "è§£å†³æ–¹æ³•:\n"
                "  pip install einops imageio ftfy\n"
                "  ç„¶åé‡å¯æœåŠ¡"
            )
        
        # ä¼˜åŒ–: èåˆæŠ•å½±å±‚
        for block in self.pipe.transformer.blocks:
            block.self_attn.fuse_projections()
        
        print("æ¨¡å‹åŠ è½½å®Œæˆï¼")
        
        self.state = None
        self.current_frames = []
        
    def initialize_generation(self, prompt, start_frame=None, num_inference_steps=4, strength=0.45, seed=None):
        """åˆå§‹åŒ–ç”Ÿæˆè¿‡ç¨‹"""
        self.state = PipelineState()
        self.current_frames = []
        
        if seed is not None:
            self.generator = torch.Generator(self.device).manual_seed(seed)
        else:
            self.generator = None
            
        self.prompt = prompt
        self.num_inference_steps = num_inference_steps
        self.strength = strength
        self.start_frame = start_frame
        self.block_idx = 0
        
    def generate_next_block(self, input_frame=None):
        """ç”Ÿæˆä¸‹ä¸€ä¸ª block çš„å¸§"""
        kwargs = {
            "state": self.state,
            "prompt": [self.prompt],
            "num_inference_steps": self.num_inference_steps,
            "strength": self.strength,
            "block_idx": self.block_idx,
        }
        
        if self.generator is not None:
            kwargs["generator"] = self.generator
            
        # å¦‚æœæ˜¯ video-to-video æˆ– webcam æ¨¡å¼ï¼Œæ·»åŠ è¾“å…¥å¸§
        if input_frame is not None:
            kwargs["video"] = input_frame
        elif self.start_frame is not None and self.block_idx == 0:
            kwargs["video"] = self.start_frame
            
        # ç”Ÿæˆ
        self.state = self.pipe(**kwargs)
        
        # æå–ç”Ÿæˆçš„å¸§
        new_frames = self.state.values["videos"][0]
        self.current_frames.extend(new_frames)
        self.block_idx += 1
        
        return new_frames
    
    def process_frame_bytes(self, frame_bytes):
        """å°†å­—èŠ‚æ•°æ®è½¬æ¢ä¸ºæ¨¡å‹å¯ç”¨çš„æ ¼å¼"""
        # å°† JPEG å­—èŠ‚è½¬æ¢ä¸º PIL Image
        image = Image.open(io.BytesIO(frame_bytes))
        # è½¬æ¢ä¸º numpy array
        frame = np.array(image)
        return frame
    
    def frame_to_bytes(self, frame):
        """å°†å¸§è½¬æ¢ä¸º JPEG å­—èŠ‚"""
        if isinstance(frame, torch.Tensor):
            # Tensor -> numpy
            frame = frame.cpu().numpy()
            # å‡è®¾èŒƒå›´æ˜¯ [-1, 1] æˆ– [0, 1]
            if frame.max() <= 1.0:
                frame = (frame * 255).astype(np.uint8)
        
        # numpy -> PIL -> bytes
        image = Image.fromarray(frame)
        buf = io.BytesIO()
        image.save(buf, format='JPEG', quality=90)
        return buf.getvalue()


# å•ä¾‹æ¨¡å¼ - é¿å…é‡å¤åŠ è½½æ¨¡å‹
_model_instance = None

def get_model(model_path=None, quantization=None):
    """è·å–æ¨¡å‹å•ä¾‹
    
    Args:
        model_path: è‡ªå®šä¹‰æ¨¡å‹è·¯å¾„ (å¯é€‰)
                   - æœ¬åœ°è·¯å¾„: "/path/to/model"
                   - HuggingFace repo: "krea/krea-realtime-video"
                   - None: ä½¿ç”¨é»˜è®¤
        quantization: é‡åŒ–ç±»å‹ (å¯é€‰)
                     - None: ä¸é‡åŒ– (éœ€è¦ ~54GB+ æ˜¾å­˜)
                     - "int8": 8ä½é‡åŒ– (éœ€è¦ ~24GB æ˜¾å­˜)
                     - "int4": 4ä½é‡åŒ– (éœ€è¦ ~12GB æ˜¾å­˜)
    """
    global _model_instance
    if _model_instance is None:
        _model_instance = KreaLocalInference(model_path=model_path, quantization=quantization)
    return _model_instance
