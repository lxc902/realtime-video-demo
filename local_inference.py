"""
KREA Realtime Video - æœ¬åœ° GPU æ¨ç†æ¨¡å—
ä½¿ç”¨ diffusers åº“åœ¨æœ¬åœ° GPU ä¸Šè¿è¡Œ KREA æ¨¡å‹
"""
import torch
from diffusers import ModularPipeline
from diffusers.modular_pipelines import PipelineState
from diffusers.utils import load_video, export_to_video
from PIL import Image
import numpy as np
import io

class KreaLocalInference:
    def __init__(self, device="cuda", dtype=torch.bfloat16, model_path=None, quantization=None):
        """åˆå§‹åŒ–æœ¬åœ° KREA æ¨¡å‹
        
        Args:
            device: è®¾å¤‡ (cuda/cpu)
            dtype: æ•°æ®ç±»å‹
            model_path: è‡ªå®šä¹‰æ¨¡å‹è·¯å¾„ï¼Œå¯ä»¥æ˜¯ï¼š
                       - æœ¬åœ°è·¯å¾„: "/path/to/model"
                       - HuggingFace repo: "krea/krea-realtime-video"
                       - None: ä½¿ç”¨é»˜è®¤ HuggingFace repo
            quantization: é‡åŒ–ç±»å‹ (None, "int8", "int4")
        """
        print("æ­£åœ¨åŠ è½½ KREA Realtime Video æ¨¡å‹...")
        self.device = device
        self.dtype = dtype
        self.quantization = quantization
        
        # ç¡®å®šæ¨¡å‹è·¯å¾„
        if model_path is None:
            # é»˜è®¤ä½¿ç”¨ HuggingFace
            repo_id = "krea/krea-realtime-video"
            print(f"ä» HuggingFace åŠ è½½: {repo_id}")
        else:
            # ä½¿ç”¨è‡ªå®šä¹‰è·¯å¾„
            repo_id = model_path
            print(f"ä»è‡ªå®šä¹‰è·¯å¾„åŠ è½½: {model_path}")
        
        # åŠ è½½æ¨¡å‹
        self.pipe = ModularPipeline.from_pretrained(repo_id, trust_remote_code=True)
        
        # å¦‚æœä½¿ç”¨é‡åŒ–ï¼Œéœ€è¦å•ç‹¬åŠ è½½ transformer
        if quantization:
            print(f"ğŸ”§ å¯ç”¨ {quantization.upper()} é‡åŒ–...")
            try:
                from diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig
                from diffusers import AutoModel
                
                if quantization == "int8":
                    quant_config = DiffusersBitsAndBytesConfig(load_in_8bit=True)
                    print("   ä½¿ç”¨ 8-bit é‡åŒ– (é¢„è®¡æ˜¾å­˜ ~24GB)")
                elif quantization == "int4":
                    quant_config = DiffusersBitsAndBytesConfig(
                        load_in_4bit=True,
                        bnb_4bit_compute_dtype=torch.bfloat16,
                        bnb_4bit_quant_type="nf4",
                    )
                    print("   ä½¿ç”¨ 4-bit é‡åŒ– (é¢„è®¡æ˜¾å­˜ ~12GB)")
                else:
                    quant_config = None
                
                if quant_config:
                    # 1. å…ˆåŠ è½½é‡åŒ–çš„ transformerï¼ˆæœ€å¤§çš„ç»„ä»¶ï¼Œçº¦ 24GB/12GBï¼‰
                    print("   [1/2] æ­£åœ¨åŠ è½½é‡åŒ– transformer...")
                    transformer_quantized = AutoModel.from_pretrained(
                        repo_id,
                        subfolder="transformer",
                        quantization_config=quant_config,
                        torch_dtype=dtype,
                        trust_remote_code=True,
                    )
                    
                    # æå‰è®¾ç½® transformerï¼Œé¿å… load_components é‡æ–°åŠ è½½
                    self.pipe.transformer = transformer_quantized
                    
                    # 2. åŠ è½½å…¶ä»–ç»„ä»¶ï¼ˆVAE, scheduler ç­‰ï¼Œç›¸å¯¹è¾ƒå°ï¼‰
                    # load_components ä¼šè·³è¿‡å·²å­˜åœ¨çš„ç»„ä»¶ï¼ˆtransformerï¼‰
                    print("   [2/2] æ­£åœ¨åŠ è½½å…¶ä»–ç»„ä»¶...")
                    self.pipe.load_components(
                        trust_remote_code=True,
                        device_map=device,
                        torch_dtype={"default": dtype, "vae": torch.float16},
                    )
                    
                    # ç¡®ä¿ transformer æ˜¯æˆ‘ä»¬çš„é‡åŒ–ç‰ˆæœ¬
                    self.pipe.transformer = transformer_quantized
                    torch.cuda.empty_cache()
                    
                    print("   âœ… é‡åŒ–æ¨¡å‹åŠ è½½å®Œæˆ")
                    
            except ImportError as e:
                print(f"   âŒ é‡åŒ–åŠ è½½å¤±è´¥: {e}")
                print("   è¯·ç¡®ä¿å®‰è£…äº† bitsandbytes: pip install bitsandbytes")
                print("   å›é€€åˆ°æ ‡å‡†åŠ è½½...")
                self.pipe.load_components(
                    trust_remote_code=True,
                    device_map=device,
                    torch_dtype={"default": dtype, "vae": torch.float16},
                )
            except Exception as e:
                print(f"   âŒ é‡åŒ–åŠ è½½å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                print("   å›é€€åˆ°æ ‡å‡†åŠ è½½...")
                try:
                    self.pipe.load_components(
                        trust_remote_code=True,
                        device_map=device,
                        torch_dtype={"default": dtype, "vae": torch.float16},
                    )
                except:
                    pass
        else:
            # æ ‡å‡†åŠ è½½ï¼ˆæ— é‡åŒ–ï¼‰
            self.pipe.load_components(
                trust_remote_code=True,
                device_map=device,
                torch_dtype={"default": dtype, "vae": torch.float16},
            )
        
        # æ£€æŸ¥å…³é”®ç»„ä»¶æ˜¯å¦åŠ è½½æˆåŠŸ
        if not hasattr(self.pipe, 'transformer') or self.pipe.transformer is None:
            raise RuntimeError(
                "âŒ æ¨¡å‹åŠ è½½å¤±è´¥: transformer ç»„ä»¶æœªæ­£ç¡®åŠ è½½\n"
                "å¯èƒ½åŸå› :\n"
                "1. ç¼ºå°‘ä¾èµ–åŒ…ï¼ˆeinops, imageio, ftfyï¼‰\n"
                "2. diffusers ç‰ˆæœ¬ä¸å…¼å®¹\n"
                "è§£å†³æ–¹æ³•:\n"
                "  pip install einops imageio ftfy\n"
                "  ç„¶åé‡å¯æœåŠ¡"
            )
        
        # ä¼˜åŒ–: èåˆæŠ•å½±å±‚ï¼ˆé‡åŒ–æ¨¡å¼ä¸‹è·³è¿‡ï¼Œå› ä¸ºä¸å…¼å®¹ï¼‰
        if not quantization:
            print("ğŸ”§ èåˆæŠ•å½±å±‚...")
            for block in self.pipe.transformer.blocks:
                block.self_attn.fuse_projections()
        else:
            print("âš ï¸  é‡åŒ–æ¨¡å¼ä¸‹è·³è¿‡ fuse_projectionsï¼ˆä¸å…¼å®¹ï¼‰")
        
        print("æ¨¡å‹åŠ è½½å®Œæˆï¼")
        
        self.state = None
        self.current_frames = []
        
    def initialize_generation(self, prompt, start_frame=None, num_inference_steps=4, strength=0.45, seed=None):
        """åˆå§‹åŒ–ç”Ÿæˆè¿‡ç¨‹"""
        self.state = PipelineState()
        self.current_frames = []
        
        if seed is not None:
            self.generator = torch.Generator(self.device).manual_seed(seed)
        else:
            self.generator = None
            
        self.prompt = prompt
        self.num_inference_steps = num_inference_steps
        self.strength = strength
        self.start_frame = start_frame
        self.block_idx = 0
        
    def generate_next_block(self, input_frame=None):
        """ç”Ÿæˆä¸‹ä¸€ä¸ª block çš„å¸§"""
        kwargs = {
            "state": self.state,
            "prompt": [self.prompt],
            "num_inference_steps": self.num_inference_steps,
            "strength": self.strength,
            "block_idx": self.block_idx,
        }
        
        if self.generator is not None:
            kwargs["generator"] = self.generator
            
        # å¦‚æœæ˜¯ video-to-video æˆ– webcam æ¨¡å¼ï¼Œæ·»åŠ è¾“å…¥å¸§
        if input_frame is not None:
            kwargs["video"] = input_frame
        elif self.start_frame is not None and self.block_idx == 0:
            kwargs["video"] = self.start_frame
            
        # ç”Ÿæˆ
        self.state = self.pipe(**kwargs)
        
        # æå–ç”Ÿæˆçš„å¸§
        new_frames = self.state.values["videos"][0]
        self.current_frames.extend(new_frames)
        self.block_idx += 1
        
        return new_frames
    
    def process_frame_bytes(self, frame_bytes):
        """å°†å­—èŠ‚æ•°æ®è½¬æ¢ä¸ºæ¨¡å‹å¯ç”¨çš„æ ¼å¼"""
        # å°† JPEG å­—èŠ‚è½¬æ¢ä¸º PIL Image
        image = Image.open(io.BytesIO(frame_bytes))
        # è½¬æ¢ä¸º numpy array
        frame = np.array(image)
        return frame
    
    def frame_to_bytes(self, frame):
        """å°†å¸§è½¬æ¢ä¸º JPEG å­—èŠ‚"""
        if isinstance(frame, torch.Tensor):
            # Tensor -> numpy
            frame = frame.cpu().numpy()
            # å‡è®¾èŒƒå›´æ˜¯ [-1, 1] æˆ– [0, 1]
            if frame.max() <= 1.0:
                frame = (frame * 255).astype(np.uint8)
        
        # numpy -> PIL -> bytes
        image = Image.fromarray(frame)
        buf = io.BytesIO()
        image.save(buf, format='JPEG', quality=90)
        return buf.getvalue()


# å•ä¾‹æ¨¡å¼ - é¿å…é‡å¤åŠ è½½æ¨¡å‹
_model_instance = None

def get_model(model_path=None, quantization=None):
    """è·å–æ¨¡å‹å•ä¾‹
    
    Args:
        model_path: è‡ªå®šä¹‰æ¨¡å‹è·¯å¾„ (å¯é€‰)
                   - æœ¬åœ°è·¯å¾„: "/path/to/model"
                   - HuggingFace repo: "krea/krea-realtime-video"
                   - None: ä½¿ç”¨é»˜è®¤
        quantization: é‡åŒ–ç±»å‹ (å¯é€‰)
                     - None: ä¸é‡åŒ– (éœ€è¦ ~54GB+ æ˜¾å­˜)
                     - "int8": 8ä½é‡åŒ– (éœ€è¦ ~24GB æ˜¾å­˜)
                     - "int4": 4ä½é‡åŒ– (éœ€è¦ ~12GB æ˜¾å­˜)
    """
    global _model_instance
    if _model_instance is None:
        _model_instance = KreaLocalInference(model_path=model_path, quantization=quantization)
    return _model_instance
