"""
KREA Realtime Video - æœ¬åœ° GPU æ¨ç†æ¨¡å—
ä½¿ç”¨ diffusers åº“åœ¨æœ¬åœ° GPU ä¸Šè¿è¡Œ KREA æ¨¡å‹
"""
import torch
from diffusers import ModularPipeline
from diffusers.modular_pipelines import PipelineState
from diffusers.utils import load_video, export_to_video
from PIL import Image
import numpy as np
import io

class KreaLocalInference:
    def __init__(self, device="cuda", dtype=torch.bfloat16, model_path=None, quantization=None):
        """åˆå§‹åŒ–æœ¬åœ° KREA æ¨¡å‹
        
        Args:
            device: è®¾å¤‡ (cuda/cpu)
            dtype: æ•°æ®ç±»å‹
            model_path: è‡ªå®šä¹‰æ¨¡å‹è·¯å¾„ï¼Œå¯ä»¥æ˜¯ï¼š
                       - æœ¬åœ°è·¯å¾„: "/path/to/model"
                       - HuggingFace repo: "krea/krea-realtime-video"
                       - None: ä½¿ç”¨é»˜è®¤ HuggingFace repo
            quantization: é‡åŒ–ç±»å‹ (None, "int8", "int4")
        """
        print("æ­£åœ¨åŠ è½½ KREA Realtime Video æ¨¡å‹...")
        self.device = device
        self.dtype = dtype
        self.quantization = quantization
        
        # ç¡®å®šæ¨¡å‹è·¯å¾„
        if model_path is None:
            # æ€»æ˜¯ä»åŸå§‹ repo åŠ è½½ pipeline ç»“æ„
            repo_id = "krea/krea-realtime-video"
            print(f"ä» HuggingFace åŠ è½½: {repo_id}")
        else:
            # ä½¿ç”¨è‡ªå®šä¹‰è·¯å¾„
            repo_id = model_path
            print(f"ä»è‡ªå®šä¹‰è·¯å¾„åŠ è½½: {model_path}")
        
        # åŠ è½½æ¨¡å‹
        self.pipe = ModularPipeline.from_pretrained(repo_id, trust_remote_code=True)
        
        # æ ¹æ®é‡åŒ–ç±»å‹åŠ è½½æ¨¡å‹
        if quantization == "fp8":
            # FP8 é¢„é‡åŒ–æ¨¡å‹
            print("ğŸ”§ ä½¿ç”¨ FP8 é¢„é‡åŒ–æ¨¡å‹ (é¢„è®¡æ˜¾å­˜ ~24GB)")
            try:
                from huggingface_hub import hf_hub_download
                
                # 1. ä¸‹è½½ FP8 transformer checkpoint
                fp8_repo = "6chan/krea-realtime-video-fp8"
                fp8_file = "krea-realtime-video-14b-fp8-e4m3fn.safetensors"
                print(f"   [1/3] ä¸‹è½½ FP8 transformer: {fp8_repo}")
                
                fp8_path = hf_hub_download(
                    repo_id=fp8_repo,
                    filename=fp8_file,
                )
                print(f"   âœ… FP8 checkpoint: {fp8_path}")
                
                # 2. åŠ è½½å…¶ä»–ç»„ä»¶ï¼ˆä¸åŒ…æ‹¬ transformerï¼‰
                print("   [2/3] åŠ è½½å…¶ä»–ç»„ä»¶...")
                config_only_components = {"transformer", "guider", "video_processor", "scheduler"}
                specs = self.pipe._component_specs
                if isinstance(specs, dict):
                    all_component_names = list(specs.keys())
                elif specs:
                    first = next(iter(specs), None)
                    if hasattr(first, 'name'):
                        all_component_names = [spec.name for spec in specs]
                    else:
                        all_component_names = list(specs)
                else:
                    all_component_names = []
                
                components_to_load = [name for name in all_component_names if name not in config_only_components]
                
                self.pipe.load_components(
                    names=components_to_load,
                    trust_remote_code=True,
                    device_map=device,
                    torch_dtype={"default": dtype, "vae": torch.float16},
                )
                
                # 3. åŠ è½½ FP8 transformer
                print("   [3/3] åŠ è½½ FP8 transformer...")
                from safetensors.torch import load_file
                
                # å…ˆåŠ è½½åŸå§‹ transformer ç»“æ„
                from diffusers import AutoModel
                transformer = AutoModel.from_pretrained(
                    repo_id,
                    subfolder="transformer",
                    torch_dtype=torch.float8_e4m3fn,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True,
                )
                
                # åŠ è½½ FP8 æƒé‡
                fp8_state_dict = load_file(fp8_path)
                transformer.load_state_dict(fp8_state_dict, strict=False)
                transformer = transformer.to(device)
                
                self.pipe.transformer = transformer
                
                torch.cuda.empty_cache()
                print("   âœ… FP8 æ¨¡å‹åŠ è½½å®Œæˆ")
                
            except Exception as e:
                print(f"   âŒ FP8 åŠ è½½å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                raise RuntimeError(f"FP8 åŠ è½½å¤±è´¥: {e}")
        elif quantization in ("int8", "int4"):
            # ä½¿ç”¨ torchao é‡åŒ–ï¼ˆæ›¿ä»£ bitsandbytesï¼Œå…¼å®¹æ€§æ›´å¥½ï¼‰
            print(f"ğŸ”§ å¯ç”¨ {quantization.upper()} é‡åŒ– (torchao)...")
            
            try:
                from torchao.quantization import quantize_, int8_dynamic_activation_int8_weight, int4_weight_only
                
                # 1. å…ˆæ ‡å‡†åŠ è½½æ‰€æœ‰ç»„ä»¶
                print("   [1/3] æ­£åœ¨åŠ è½½æ¨¡å‹ç»„ä»¶...")
                self.pipe.load_components(
                    trust_remote_code=True,
                    device_map=device,
                    torch_dtype={"default": dtype, "vae": torch.float16},
                )
                
                # 2. å®šä¹‰é‡åŒ–è¿‡æ»¤å™¨ï¼šåªé‡åŒ– Linear å±‚ï¼Œè·³è¿‡ Conv2D
                def linear_only_filter(module, name):
                    return isinstance(module, torch.nn.Linear)
                
                # 3. å¯¹ transformer è¿›è¡Œé‡åŒ–
                print("   [2/3] æ­£åœ¨é‡åŒ– transformer (ä»… Linear å±‚)...")
                if quantization == "int8":
                    print("   ä½¿ç”¨ INT8 åŠ¨æ€é‡åŒ– (é¢„è®¡æ˜¾å­˜ ~28GB)")
                    quantize_(
                        self.pipe.transformer, 
                        int8_dynamic_activation_int8_weight(),
                        filter_fn=linear_only_filter
                    )
                else:  # int4
                    print("   ä½¿ç”¨ INT4 æƒé‡é‡åŒ– (é¢„è®¡æ˜¾å­˜ ~16GB)")
                    quantize_(
                        self.pipe.transformer,
                        int4_weight_only(),
                        filter_fn=linear_only_filter
                    )
                
                # 4. æ¸…ç†æ˜¾å­˜
                print("   [3/3] æ¸…ç†æ˜¾å­˜ç¼“å­˜...")
                torch.cuda.empty_cache()
                print("   âœ… torchao é‡åŒ–å®Œæˆ")
                
            except ImportError as e:
                print(f"   âŒ é‡åŒ–å¤±è´¥: {e}")
                print("   è¯·å®‰è£… torchao: pip install torchao")
                raise RuntimeError(f"é‡åŒ–å¤±è´¥ï¼Œç¼ºå°‘ä¾èµ–: {e}")
            except Exception as e:
                print(f"   âŒ é‡åŒ–å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                raise RuntimeError(f"é‡åŒ–å¤±è´¥: {e}")
        else:
            # æ ‡å‡†åŠ è½½ï¼ˆæ— é‡åŒ–ï¼‰
            self.pipe.load_components(
                trust_remote_code=True,
                device_map=device,
                torch_dtype={"default": dtype, "vae": torch.float16},
            )
        
        # æ£€æŸ¥å…³é”®ç»„ä»¶æ˜¯å¦åŠ è½½æˆåŠŸ
        if not hasattr(self.pipe, 'transformer') or self.pipe.transformer is None:
            raise RuntimeError(
                "âŒ æ¨¡å‹åŠ è½½å¤±è´¥: transformer ç»„ä»¶æœªæ­£ç¡®åŠ è½½\n"
                "å¯èƒ½åŸå› :\n"
                "1. ç¼ºå°‘ä¾èµ–åŒ…ï¼ˆeinops, imageio, ftfyï¼‰\n"
                "2. diffusers ç‰ˆæœ¬ä¸å…¼å®¹\n"
                "è§£å†³æ–¹æ³•:\n"
                "  pip install einops imageio ftfy\n"
                "  ç„¶åé‡å¯æœåŠ¡"
            )
        
        # ä¼˜åŒ–: èåˆæŠ•å½±å±‚ï¼ˆé‡åŒ–æ¨¡å¼ä¸‹è·³è¿‡ï¼Œå› ä¸ºå¯èƒ½ä¸å…¼å®¹ï¼‰
        if not quantization:
            print("ğŸ”§ èåˆæŠ•å½±å±‚...")
            for block in self.pipe.transformer.blocks:
                block.self_attn.fuse_projections()
        elif quantization == "fp8":
            # FP8 å¯ä»¥å°è¯• fuse_projectionsï¼Œä½†å¦‚æœå¤±è´¥å°±è·³è¿‡
            try:
                print("ğŸ”§ å°è¯•èåˆæŠ•å½±å±‚...")
                for block in self.pipe.transformer.blocks:
                    block.self_attn.fuse_projections()
                print("   âœ… èåˆæˆåŠŸ")
            except Exception as e:
                print(f"   âš ï¸  è·³è¿‡ fuse_projections: {e}")
        else:
            print("âš ï¸  é‡åŒ–æ¨¡å¼ä¸‹è·³è¿‡ fuse_projectionsï¼ˆä¸å…¼å®¹ï¼‰")
        
        print("æ¨¡å‹åŠ è½½å®Œæˆï¼")
        
        self.state = None
        self.current_frames = []
        
    def initialize_generation(self, prompt, start_frame=None, num_inference_steps=4, strength=0.45, seed=None):
        """åˆå§‹åŒ–ç”Ÿæˆè¿‡ç¨‹"""
        self.state = PipelineState()
        self.current_frames = []
        
        if seed is not None:
            self.generator = torch.Generator(self.device).manual_seed(seed)
        else:
            self.generator = None
            
        self.prompt = prompt
        self.num_inference_steps = num_inference_steps
        self.strength = strength
        self.start_frame = start_frame
        self.block_idx = 0
        
    def generate_next_block(self, input_frame=None):
        """ç”Ÿæˆä¸‹ä¸€ä¸ª block çš„å¸§"""
        kwargs = {
            "state": self.state,
            "prompt": [self.prompt],
            "num_inference_steps": self.num_inference_steps,
            "strength": self.strength,
            "block_idx": self.block_idx,
        }
        
        if self.generator is not None:
            kwargs["generator"] = self.generator
            
        # å¦‚æœæ˜¯ video-to-video æˆ– webcam æ¨¡å¼ï¼Œæ·»åŠ è¾“å…¥å¸§
        if input_frame is not None:
            kwargs["video"] = input_frame
        elif self.start_frame is not None and self.block_idx == 0:
            kwargs["video"] = self.start_frame
            
        # ç”Ÿæˆ
        self.state = self.pipe(**kwargs)
        
        # æå–ç”Ÿæˆçš„å¸§
        new_frames = self.state.values["videos"][0]
        self.current_frames.extend(new_frames)
        self.block_idx += 1
        
        return new_frames
    
    def process_frame_bytes(self, frame_bytes):
        """å°†å­—èŠ‚æ•°æ®è½¬æ¢ä¸ºæ¨¡å‹å¯ç”¨çš„æ ¼å¼"""
        # å°† JPEG å­—èŠ‚è½¬æ¢ä¸º PIL Image
        image = Image.open(io.BytesIO(frame_bytes))
        # è½¬æ¢ä¸º numpy array
        frame = np.array(image)
        return frame
    
    def frame_to_bytes(self, frame):
        """å°†å¸§è½¬æ¢ä¸º JPEG å­—èŠ‚"""
        if isinstance(frame, torch.Tensor):
            # Tensor -> numpy
            frame = frame.cpu().numpy()
            # å‡è®¾èŒƒå›´æ˜¯ [-1, 1] æˆ– [0, 1]
            if frame.max() <= 1.0:
                frame = (frame * 255).astype(np.uint8)
        
        # numpy -> PIL -> bytes
        image = Image.fromarray(frame)
        buf = io.BytesIO()
        image.save(buf, format='JPEG', quality=90)
        return buf.getvalue()


# å•ä¾‹æ¨¡å¼ - é¿å…é‡å¤åŠ è½½æ¨¡å‹
_model_instance = None

def get_model(model_path=None, quantization=None):
    """è·å–æ¨¡å‹å•ä¾‹
    
    Args:
        model_path: è‡ªå®šä¹‰æ¨¡å‹è·¯å¾„ (å¯é€‰)
                   - æœ¬åœ°è·¯å¾„: "/path/to/model"
                   - HuggingFace repo: "krea/krea-realtime-video"
                   - None: ä½¿ç”¨é»˜è®¤
        quantization: é‡åŒ–ç±»å‹ (å¯é€‰)
                     - None: ä¸é‡åŒ– (éœ€è¦ ~54GB+ æ˜¾å­˜)
                     - "int8": 8ä½é‡åŒ– (éœ€è¦ ~24GB æ˜¾å­˜)
                     - "int4": 4ä½é‡åŒ– (éœ€è¦ ~12GB æ˜¾å­˜)
    """
    global _model_instance
    if _model_instance is None:
        _model_instance = KreaLocalInference(model_path=model_path, quantization=quantization)
    return _model_instance
